{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Pyspark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9GlkGWVhPvL7oISBToNTC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverRevilla/Spark_Pyspark/blob/main/Feature_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GlBtUqFYIh"
      },
      "source": [
        "## **Spark Changes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTbbFxIeFiaM"
      },
      "source": [
        "# return spark version\n",
        "spark.version\n",
        "\n",
        "# return python version\n",
        "import sys\n",
        "sys.version_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XCH3572GIdA"
      },
      "source": [
        "## **Verifying DataTypes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ8Vm7LSGLsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b187cfc5-b224-4038-c07b-bb1d15e814db"
      },
      "source": [
        "# Verifying DataTypes\n",
        "\"\"\"\n",
        "# create list of actual dtypes to check\n",
        "actual_dtypes_list = df.dtypes\n",
        "print(actual_dtypes_list)\n",
        "\n",
        "# Iterate through the list of actual dtypes tuples\n",
        "for attribute_tuple in actual_dtypes_list:\n",
        "  \n",
        "  # Check if column name is dictionary of expected dtypes\n",
        "  col_name = attribute_tuple[0]\n",
        "  if col_name in validation_dict:\n",
        "\n",
        "    # Compare attribute types\n",
        "    col_type = attribute_tuple[1]\n",
        "    if col_type == validation_dict[col_name]:\n",
        "      print(col_name + ' has expected dtype.')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# create list of actual dtypes to check\\nactual_dtypes_list = df.dtypes\\nprint(actual_dtypes_list)\\n\\n# Iterate through the list of actual dtypes tuples\\nfor attribute_tuple in actual_dtypes_list:\\n  \\n  # Check if column name is dictionary of expected dtypes\\n  col_name = attribute_tuple[0]\\n  if col_name in validation_dict:\\n\\n    # Compare attribute types\\n    col_type = attribute_tuple[1]\\n    if col_type == validation_dict[col_name]:\\n      print(col_name + ' has expected dtype.')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QGFUlDfFVRb"
      },
      "source": [
        "## **Visually Inspecting Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07joAU7jE5k7"
      },
      "source": [
        "# df.describe(['Name_column','Name_column2',...]).show()\n",
        "# pyspark.sql.functions.mean(col)\n",
        "# pyspark.sql.functions.skewness(col)  #lack of symmetry\n",
        "# pyspark.sql.functions.min(col)\n",
        "# cov(col1,col2)\n",
        "# corr(col1,col2)\n",
        "\n",
        "# pyspark.sql.functions.mean(col)\n",
        "df.agg({'SALESCLOSEPRICE':'mean'}).collect()\n",
        "\n",
        "# cov(col1,col2)\n",
        "df.cov('SALESCLOSEPRICE','YEARBUILT')\n",
        "\n",
        "# Sample PySpark DataFrames before converting to Pandas\n",
        "df.sample(False,0.5,42).count()\n",
        "sns.distplot(a)\n",
        "\n",
        "# Prepping for plotting a distribution\n",
        "import seaborn as sns\n",
        "sample_df = df.select(['SALESCLOSEPRICE']).sample(False,0.5,42) # 0.5 50% of the data, randomseed = 42\n",
        "pandas_df = sample_df.toPandas()\n",
        "sns.distplot(pandas_df)\n",
        "\n",
        "# Checking the maximun correlation value of all columns with 'SALESCLOSEPRICE'\n",
        "# Name and value of col with max corr\n",
        "corr_max = 0\n",
        "corr_max_col = columns[0]\n",
        "\n",
        "# Loop to check all columns contained in list\n",
        "for col in columns:\n",
        "    # Check the correlation of a pair of columns\n",
        "    corr_val = df.corr(col,'SALESCLOSEPRICE')\n",
        "    # Logic to compare corr_max with current corr_val\n",
        "    if corr_val > corr_max:\n",
        "        # Update the column name and corr value\n",
        "        corr_max = corr_val\n",
        "        corr_max_col = col\n",
        "\n",
        "print(corr_max_col)\n",
        "\n",
        "\n",
        "# Select a single column and sample and convert to pandas\n",
        "sample_df = df.select(['LISTPRICE']).sample(False, 0.5, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "# Plot distribution of pandas_df and display plot\n",
        "sns.distplot(pandas_df)\n",
        "plt.show()\n",
        "# Import skewness function\n",
        "from pyspark.sql.functions import skewness\n",
        "# Compute and print skewness of LISTPRICE\n",
        "print(df.agg({'LISTPRICE': 'skewness'}).collect())\n",
        "\n",
        "#Using Visualizations: Implot\n",
        "\n",
        "# Select a the relevant columns and sample\n",
        "sample_df = df.select(['SALESCLOSEPRICE', 'LIVINGAREA']).sample(False, 0.5, 42)\n",
        "\n",
        "# Convert to pandas dataframe\n",
        "pandas_df = sample_df.toPandas()\n",
        "\n",
        "# Linear model plot of pandas_df\n",
        "sns.lmplot(x= 'LIVINGAREA', y= 'SALESCLOSEPRICE', data= pandas_df)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bl-bySwpzc_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VvlZHXp1e6"
      },
      "source": [
        "# **Wrangling with Spark Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMPE3yMQp_eZ"
      },
      "source": [
        "**Dropping data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLfPkhVp9OV"
      },
      "source": [
        "# List of columns to drop\n",
        "cols_to_drop = ['NO','UNITNUMBER','CLASS']\n",
        "# Drop columns\n",
        "df = df.drop(*cols_to_drop)\n",
        "\n",
        "# Show top 30 records\n",
        "df.show(30)\n",
        "\n",
        "# List of columns to remove from dataset\n",
        "cols_to_drop = ['STREETNUMBERNUMERIC','LOTSIZEDIMENSIONS']\n",
        "\n",
        "# Drop columns in list\n",
        "df = df.drop(*cols_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZQC0ERRtZti"
      },
      "source": [
        "# Text filtering\n",
        "where(condicion)\n",
        "like(expression)\n",
        "isin(lista)\n",
        "df = df.where(df['POTENCIALSHORTSALE'].like('Not Disclosed'))\n",
        "\n",
        "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
        "df.select(['ASSUMABLEMORTGAGE']).distinct().show()\n",
        "\n",
        "# List of possible values containing 'yes'\n",
        "yes_values = ['Yes w/ Qualifying','Yes w/No Qualifying']\n",
        "\n",
        "# Filter the text values out of df but keep null values\n",
        "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
        "df = df.where(text_filter)\n",
        "\n",
        "# Print count of remaining records\n",
        "print(df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb269ueCtcMm"
      },
      "source": [
        "# Value filtering\n",
        "\n",
        "# Filtering outliers\n",
        "std_val = df.agg({'SALESCLOSEPRICE':'stddev'}).collect()[0][0]\n",
        "mean_val = df.agg({'SALESCLOSEPRICE':'mean'}).collect()[0][0]\n",
        "hi_bound = mean_val + (3*std_val)\n",
        "low_bound = mean_val - (3*std_val)\n",
        "df = df.where((df['LISTPRICE'] < hi_bound) & (df['LISTPRICE'] > low_bound))\n",
        "\n",
        "from pyspark.sql.functions import mean,stddev\n",
        "\n",
        "# Calculate values used for outlier filtering\n",
        "mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]\n",
        "stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]\n",
        "\n",
        "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
        "low_bound = mean_val - (3 * stddev_val)\n",
        "hi_bound = mean_val + (3 * stddev_val)\n",
        "\n",
        "# Filter the data to fit between the lower and upper bounds\n",
        "df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbz_KVgRtexj"
      },
      "source": [
        "# Dropping NA's or Nulls\n",
        "#.dropna(how = , thresh = ,subset =)\n",
        "DataFrame.dropna()\n",
        "# Drop any records with NULL Values\n",
        "df = df.dropna()\n",
        "# Drop records if both LISTPRICE and SALESPRICE are NULL\n",
        "df = df.dropna(how = 'all', subset = ['LISTPRICE','SALESCLOSEPRICE'])\n",
        "# Drop records where at least two columns have NULL values\n",
        "df = df.dropna(thresh = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHkn7m0ItheW"
      },
      "source": [
        "# Removing Duplicates\n",
        ".dropDuplicates()\n",
        "# Entire DataFrame\n",
        "df.dropDuplicates()\n",
        "# Check only a columns list\n",
        "df.dropDuplicates(['streetaddress'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wURuf-ix9lH"
      },
      "source": [
        "**Adjusting Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ0mWlIXyCm5"
      },
      "source": [
        "# Scaling data\n",
        "# Minmax scaling\n",
        "max_days = df.agg({'DAYSONMARKET':'max'}).collect()[0][0]\n",
        "min_days = df.agg({'DAYSONMARKET':'min'}).collect()[0][0]\n",
        "df = df.withColumn(\"scaled_days\",\n",
        "                   (df['DAYSONMARKET']- min_days)/(max_days - min_days))\n",
        "df[['scales_days']].show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjyVR0G0PwA"
      },
      "source": [
        "# Standarization: Transform the data to standard normal distribution\n",
        "mean_days = df.agg({'DAYSONMARKET':'mean'}).collect()[0][0]\n",
        "stddev_days = df.agg({'DAYSONMARKET':'stddev'}).collect()[0][0]\n",
        "df = df.withColumn('ztrams_days',\n",
        "                   (df['DAYSONMARKET']- mean_days)/stddev_days)\n",
        "df.agg({'ztrans_days':'mean'}).collect()\n",
        "df.agg({'ztrans_days':'stddev'}).collect()\n",
        "\n",
        "# Define max and min values and collect them\n",
        "max_days = df.agg({'DAYSONMARKET': 'max'}).collect()[0][0]\n",
        "min_days = df.agg({'DAYSONMARKET': 'min'}).collect()[0][0]\n",
        "\n",
        "# Create a new column based off the scaled data\n",
        "df = df.withColumn('percentage_scaled_days', \n",
        "                  round((df['DAYSONMARKET'] - min_days) / (max_days - min_days)) * 100)\n",
        "\n",
        "# Calc max and min for new column\n",
        "print(df.agg({'percentage_scaled_days': 'max'}).collect())\n",
        "print(df.agg({'percentage_scaled_days': 'min'}).collect())\n",
        "\n",
        "## Function to scale\n",
        "def min_max_scaler(df, cols_to_scale):\n",
        "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
        "  for col in cols_to_scale:\n",
        "    # Define min and max values and collect them\n",
        "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
        "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
        "    new_column_name = 'scaled_' + col\n",
        "    # Create a new column based off the scaled data\n",
        "    df = df.withColumn(new_column_name, \n",
        "                      (df[col] - min_days) / (max_days - min_days))\n",
        "  return df\n",
        "  \n",
        "df = min_max_scaler(df, cols_to_scale)\n",
        "# Show that our data is now between 0 and 1\n",
        "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFIsAHIc0ROA"
      },
      "source": [
        "# Log scaling\n",
        "from pyspark.sql.functions import log\n",
        "df = df.withColumn('log_SalesClosePrice',log(df['SALESCLOSEPRICE']))\n",
        "\n",
        "from pyspark.sql.functions import log\n",
        "\n",
        "# Compute the skewness\n",
        "print(df.agg({'YEARBUILT': 'skewness'}).collect())\n",
        "\n",
        "# Calculate the max year\n",
        "max_year = df.agg({'YEARBUILT': 'max'}).collect()[0][0]\n",
        "\n",
        "# Create a new column of reflected data\n",
        "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
        "\n",
        "# Create a new column based reflected data\n",
        "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZZEpjHBKEPI"
      },
      "source": [
        "**Working with missing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI7rOkfbKI27"
      },
      "source": [
        "# Assessing Missing Values\n",
        "# isNull()\n",
        "df.where(df['ROOF'].isNull()).count()\n",
        "\n",
        "# Function to automate dropping\n",
        "def column_dropper(df, threshold):\n",
        "  # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
        "  total_records = df.count()\n",
        "  for col in df.columns:\n",
        "    # Calculate the percentage of missing values\n",
        "    missing = df.where(df[col].isNull()).count()\n",
        "    missing_percent = missing / total_records\n",
        "    # Drop column if percent of missing is more than threshold\n",
        "    if missing_percent > threshold:\n",
        "      df = df.drop(col)\n",
        "  return df\n",
        "\n",
        "# Drop columns that are more than 60% missing\n",
        "df = column_dropper(df, 0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1bXAMATNSD-"
      },
      "source": [
        "# Plotting Missing Values\n",
        "import seaborn as sns\n",
        "sub_df = df.select(['ROOMAREA1'])\n",
        "sample_df = sub_df.sample(False,0.5,4)\n",
        "pandas_df = sample_df.toPandas()\n",
        "sns.heatmap(data = pandas.isnull())\n",
        "\n",
        "# Sample the dataframe and convert to Pandas\n",
        "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "\n",
        "# Convert all values to T/F\n",
        "tf_df = pandas_df.isnull()\n",
        "\n",
        "# Plot it\n",
        "sns.heatmap(data=tf_df)\n",
        "plt.xticks(rotation=30, fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# Set the answer to the column with the most missing data\n",
        "answer = 'BACKONMARKETDATE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2T_IqqrNTy8"
      },
      "source": [
        "# Imputation of missing values\n",
        "# fillna(value, subset = None): \n",
        "#value: the value to replace missing with\n",
        "#subset: the list of column names to replace missing\n",
        "# Replacing missing values with zero\n",
        "df.fillna(0, subset = ['DAYSONMARKET'])\n",
        "\n",
        "# Replacing with the mean value for that column\n",
        "col_mean = df.agg({'DAYSONMARKET':'mean'}).collect()[0][0]\n",
        "df.fillna(col_mean,subset = ['DAYSONMARKET'])\n",
        "\n",
        "# Sample the dataframe and convert to Pandas\n",
        "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "\n",
        "# Convert all values to T/F\n",
        "tf_df = pandas_df.isnull()\n",
        "\n",
        "# Plot it\n",
        "sns.heatmap(data=tf_df)\n",
        "plt.xticks(rotation=30, fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# Set the answer to the column with the most missing data\n",
        "answer = 'BACKONMARKETDATE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMpexXfWT1eO"
      },
      "source": [
        "**Joins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaU0ZoZ4T4mi"
      },
      "source": [
        "# Pyspark DataFrame Joins\n",
        "DataFrame.join(\n",
        "    other, # Other DataFrame to merge\n",
        "    on = None, # The keys to join on\n",
        "    how = None # Type of join to perform (default is 'inner')\n",
        ")\n",
        "\n",
        "# Example:\n",
        "hdf.show(2)\n",
        "# Specify the condition\n",
        "cond = [df['OFFMARKETDATE'] == hdf['dt']]\n",
        "# Join two hdf onto df\n",
        "df = df.join(hdf, on = cond, 'left')\n",
        "# How many sales occurred on bank holidays\n",
        "df.where(df['nm'].isNull()).count()\n",
        "\n",
        "# Cast data types\n",
        "walk_df = walk_df.withColumn('longitude', walk_df['longitude'].cast('double'))\n",
        "walk_df = walk_df.withColumn('latitude', walk_df['latitude'].cast('double'))\n",
        "\n",
        "# Round precision\n",
        "df = df.withColumn('longitude', round('longitude', 5))\n",
        "df = df.withColumn('latitude', round('latitude', 5))\n",
        "\n",
        "# Create join condition\n",
        "condition = [walk_df['latitude'] == df['latitude'], walk_df['longitude'] == df['longitude']]\n",
        "\n",
        "# Join the dataframes together\n",
        "join_df = df.join(walk_df, on=condition, how='left')\n",
        "# Count non-null records from new field\n",
        "print(join_df.where(~join_df['walkscore'].isNull()).count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlOyH0QNV73E"
      },
      "source": [
        "# Spak SQL Join\n",
        "# Register the dataframe as a temp table\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "hdf.createOrReplaceTempView('hdf')\n",
        "# Write a SQL statement\n",
        "sql_df = spark.sql(\"\"\"\n",
        "    SELECT * \n",
        "    FROM df\n",
        "    LEFT JOIN hdf\n",
        "    ON df.OFFMARKETDATE =  hdf.dt \n",
        "\"\"\")\n",
        "\n",
        "# Register dataframes as tables\n",
        "df.createOrReplaceTempView('df')\n",
        "walk_df.createOrReplaceTempView('walk_df')\n",
        "\n",
        "# SQL to join dataframes\n",
        "join_sql = \t\"\"\"\n",
        "\t\t\tSELECT \n",
        "\t\t\t\t*\n",
        "\t\t\tFROM df\n",
        "\t\t\tLEFT JOIN walk_df\n",
        "\t\t\tON df.longitude = walk_df.longitude\n",
        "\t\t\tAND df.latitude = walk_df.latitude\n",
        "\t\t\t\"\"\"\n",
        "# Perform sql join\n",
        "joined_df = spark.sql(join_sql)\n",
        "\n",
        "# Join on mismatched keys precision \n",
        "wrong_prec_cond = [walk_df['longitude'] == df_orig['longitude'], walk_df['latitude'] == df_orig['latitude']]\n",
        "wrong_prec_df = df_orig.join(walk_df, on= wrong_prec_cond, how='left')\n",
        "\n",
        "# Compare bad join to the correct one\n",
        "print(wrong_prec_df.where(wrong_prec_df['walkscore'].isNull()).count())\n",
        "print(correct_join_df.where(correct_join_df['walkscore'].isNull()).count())\n",
        "\n",
        "# Create a join on too few keys\n",
        "few_keys_cond = [df['longitude'] == walk_df['longitude']]\n",
        "few_keys_df = df.join(walk_df, on=few_keys_cond, how='left')\n",
        "\n",
        "# Compare bad join to the correct one\n",
        "print(\"Record Count of the Too Few Keys Join Example: \" + str(few_keys_df.count()))\n",
        "print(\"Record Count of the Correct Join Example: \" + str(correct_join_df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq9FOTS7bMp8"
      },
      "source": [
        "# **Feature generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn9cynHNbR5F"
      },
      "source": [
        "# Creating a new feature area, by multiplicating\n",
        "df = df.column('TSQTF', (df['WIDTH']*df['LENGTH']))\n",
        "\n",
        "# Diference two date columns\n",
        "df = df.withColumn('DAYSONMARKET', datediff('OFFMARKETDATE','LISTDATE'))\n",
        "\n",
        "# Check the python libraries FeatureTools and TSFresh\n",
        "\n",
        "# ---- Exercises -----------------------------------------------------------------------------------------\n",
        "# Lot size in square feet\n",
        "acres_to_sqfeet = 43560\n",
        "df = df.withColumn('LOT_SIZE_SQFT', df['ACRES']*acres_to_sqfeet)\n",
        "\n",
        "# Create a new column YARD_SIZE\n",
        "df = df.withColumn('YARD_SIZE',df['LOT_SIZE_SQFT'] - df['FOUNDATIONSIZE'])\n",
        "\n",
        "# Corr of ACRES vs SALESCLOSEPRICE\n",
        "print(\"Corr of ACRES vs SALESCLOSEPRICE: \" + str(df.corr('ACRES','SALESCLOSEPRICE')))\n",
        "\n",
        "# Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE\n",
        "print(\"Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE: \" + str(df.corr('FOUNDATIONSIZE', 'SALESCLOSEPRICE')))\n",
        "\n",
        "# Corr of YARD_SIZE vs SALESCLOSEPRICE\n",
        "print('Corr of YARD_SIZE vs SALESCLOSEPRICE: ' + str(df.corr('YARD_SIZE','SALESCLOSEPRICE')))\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# ASSESSED_TO_LIST\n",
        "df = df.withColumn('ASSESSED_TO_LIST', (df['ASSESSEDVALUATION']/df['LISTPRICE']))\n",
        "df[['ASSESSEDVALUATION', 'LISTPRICE', 'ASSESSED_TO_LIST']].show(5)\n",
        "# TAX_TO_LIST\n",
        "df = df.withColumn('TAX_TO_LIST',(df['TAXES']/df['LISTPRICE']))\n",
        "df[['TAX_TO_LIST', 'TAXES', 'LISTPRICE']].show(5)\n",
        "# BED_TO_BATHS\n",
        "df = df.withColumn('BED_TO_BATHS',(df['BEDROOMS']/df['BATHSTOTAL']))\n",
        "df[['BED_TO_BATHS', 'BEDROOMS', 'BATHSTOTAL']].show(5)\n",
        "\n",
        "# --------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYgwBjB9OhfN"
      },
      "source": [
        "Time features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6itvcaOkmp"
      },
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Cast the data type to Date\n",
        "# to_date()\n",
        "df = df.withColumn('LISTDATE', to_date('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import year_month\n",
        "# Create a new column of year number\n",
        "df = df.withColumn('LIST_YEAR',year('LISTDATE'))\n",
        "\n",
        "# Create a new column of month number\n",
        "df = df.withColumn('LIST_MONTH', month('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import dayofmonth, weekofyear\n",
        "\n",
        "# Create new columns of the day number within the month\n",
        "df = df.withColumns('LIST_DAYOFMONTH', dayofmonth('LISTDATE'))\n",
        "\n",
        "# Create new columns of the week number within the year\n",
        "df = df.withColumn('LIST_WEEKOFYEAR', weekofyear('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import datediff\n",
        "\n",
        "# Calculate difference between two date fields\n",
        "df.withColumn('DAYSONMARKET', datediff('OFFMARKETDATE','LISTDATE'))\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import lag\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create Window\n",
        "w = Window().orderBy(m_df['DATE'])\n",
        "# Create lagged column\n",
        "m_df = m_df.withColumn('MORTAGE-1wk', lag('MORTAGE', count = 1).over(w))\n",
        "\n",
        "# ----- Exercises --------------------\n",
        "# Import needed fucntions\n",
        "from pyspark.sql.functions import to_date,dayofweek\n",
        "\n",
        "# Convert to date type\n",
        "df = df.withColumn('LISTDATE',to_date('LISTDATE'))\n",
        "\n",
        "# Get the day of the week\n",
        "df = df.withColumn('List_Day_of_Week',dayofweek('LISTDATE'))\n",
        "\n",
        "# Sample and convert to pandas dataframe\n",
        "sample_df = df.sample(False,0.5,42).toPandas()\n",
        "\n",
        "# Plot caount plot of of day of week\n",
        "sns.countplot(x='List_Day_of_Week', data = sample_df )\n",
        "plt.show()\n",
        "#--------------------------------------\n",
        "from pyspark.sql.functions import year\n",
        "\n",
        "# Initialize dataframes\n",
        "df = real_estate_df\n",
        "price_df = median_prices_df\n",
        "\n",
        "# Create year column\n",
        "df = df.withColumn('list_year', year('LISTDATE'))\n",
        "\n",
        "# Adjust year to match\n",
        "df = df.withColumn('report_year',(df['list_year'] - 1))\n",
        "\n",
        "# Create join condition\n",
        "condition = [df['CITY'] == price_df['City'], df['report_year'] == price_df['Year']]\n",
        "\n",
        "# Join the dataframes together\n",
        "df = df.join(price_df, on=condition, how= 'left')\n",
        "# Inspect that new columns are available\n",
        "df[['MedianHomeValue']].show()\n",
        "\n",
        "# -----------------------------------------\n",
        "from pyspark.sql.functions import lag, datediff, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Cast data type\n",
        "mort_df = mort_df.withColumn('DATE', to_date('DATE'))\n",
        "\n",
        "# Create window\n",
        "w = Window().orderBy(mort_df['DATE'])\n",
        "# Create lag column\n",
        "mort_df = mort_df.withColumn('DATE-1', lag('DATE', count=1).over(w))\n",
        "\n",
        "# Calculate difference between date columns\n",
        "mort_df = mort_df.withColumn('Days_Between_Report', datediff('DATE', 'DATE-1'))\n",
        "# Print results\n",
        "mort_df.select('Days_Between_Report').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF7Fcsev4zn_"
      },
      "source": [
        "Extracting features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU5iqC4t43Sw"
      },
      "source": [
        "# Extracting features to apply machine learning\n",
        "\n",
        "# Creating a boolean column\n",
        "from pyspark.sql.functions import when \n",
        "# boolean filters\n",
        "find_under_8 = df['ROOF'].like('%Age 8 Years or Less%')\n",
        "find_over_8 = df['ROOF'].like('%Age Over 8 Years%')\n",
        "\n",
        "# boolean filter 2th method\n",
        "df = df.withColumn('old',(when(find_over_8,1)\n",
        "                          .when(find_under_8,0)\n",
        "                          .otherwise(None)))\n",
        "# ---------------------------------------------------------------------\n",
        "# Import needed functions\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Create boolean conditions for string matches\n",
        "has_attached_garage = df['GARAGEDESCRIPTION'].like('%Attached Garage%')\n",
        "has_detached_garage = df['GARAGEDESCRIPTION'].like('%Detached Garage%')\n",
        "\n",
        "# Conditional value assignment \n",
        "df = df.withColumn('has_attached_garage', (when(has_attached_garage, 1)\n",
        "                                          .when(has_detached_garage, 0)\n",
        "                                          .otherwise(None)))\n",
        "\n",
        "# Inspect results\n",
        "df[['GARAGEDESCRIPTION', 'has_attached_garage']].show(truncate=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsXAp4y2_Lzz"
      },
      "source": [
        "# Spliting Columns\n",
        "from pyspark.sql.functions import split\n",
        "split_col = split(df['ROOF'],',')\n",
        "# Put the first value of the list into a new column\n",
        "df = df.withColumn('Roof_Material',split_col.getItem(0))\n",
        "# Inspect results\n",
        "df[['ROOF','Roof_Material']].show(5, truncate = 100)\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# Import needed functions\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Create boolean conditions for string matches\n",
        "has_attached_garage = df['GARAGEDESCRIPTION'].like('%Attached Garage%')\n",
        "has_detached_garage = df['GARAGEDESCRIPTION'].like('%Detached Garage%')\n",
        "\n",
        "# Conditional value assignment \n",
        "df = df.withColumn('has_attached_garage', (when(has_attached_garage, 1)\n",
        "                                          .when(has_detached_garage, 0)\n",
        "                                          .otherwise(None)))\n",
        "\n",
        "# Inspect results\n",
        "df[['GARAGEDESCRIPTION', 'has_attached_garage']].show(truncate=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH5u-5lQ_O5o"
      },
      "source": [
        "# Explode & Pivot\n",
        "from pyspark.sql.functions import split, explode,lit,coalesce,first\n",
        "  # Process\n",
        "    # Split the column on commas into a list\n",
        "    df = df.withColumn('roof_list', split(df['ROOF'],','))\n",
        "\n",
        "    # Explode list into new records for each value\n",
        "    ex_df = df.withColumn('ex_roof_list', explode(df['roof_list']))\n",
        "\n",
        "    # Creating a dummy column of constant value to pivot later\n",
        "    ex_df = ex_df.withColumn('constant_val',lit(1))\n",
        "\n",
        "    # Pivot the values into boolean columns\n",
        "    piv_df = ex_df.groupBy('NO').pivot('ex_roof_list')\\\n",
        "              .agg(coalesce(first('constant_val')))\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "from pyspark.sql.functions import coalesce, first\n",
        "\n",
        "# Pivot \n",
        "piv_df = ex_df.groupBy('NO').pivot('ex_garage_list').agg(coalesce(first('constant_val')))\n",
        "\n",
        "# Join the dataframes together and fill null\n",
        "joined_df = df.join(piv_df, on='NO', how='left')\n",
        "\n",
        "# Columns to zero fill\n",
        "zfill_cols = piv_df.columns\n",
        "\n",
        "# Zero fill the pivoted values\n",
        "zfilled_df = joined_df.fillna(0, subset= zfill_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3TNt9qjUpfI"
      },
      "source": [
        "Binarizing, Bucketing, Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6uunTihUuz6"
      },
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "# Cast the data type to double\n",
        "df = df.withColumn('FIREPLACES',df['FIREPLACES'].cast('double'))\n",
        "# Create binarizer transformer\n",
        "bin = Binarizer(threshold = 0.0,inputCol = 'FIREPLACES', outputCol = 'FirePlaceT')\n",
        "# Apply the transformer\n",
        "df = bin.transform(df)\n",
        "# Inspect the results\n",
        "df[['FIREPLACES','FireplaceT']].show(3)\n",
        "# --------------------------------------------------\n",
        "from pyspark.ml.feature import Binarizer\n",
        "# Create the transformer\n",
        "binarizer = Binarizer(threshold = 5.0,inputCol= 'List_Day_of_Week', outputCol='Listed_On_Weekend')\n",
        "\n",
        "# Apply the transformation to df\n",
        "df = binarizer.transform(df)\n",
        "\n",
        "# Verify transformation\n",
        "df[['List_Day_of_Week', 'Listed_On_Weekend']].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ClM0EPgZ0Xo"
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "# Define how to split data --- float('Inf'): infinitve value\n",
        "splits = [0,1,2,3,4, float('Inf')]\n",
        "# Create bucketing transformer\n",
        "buck = Bucketizer(splits = splits, inputCol = 'BATHSTOTAL', outputCol = 'baths')\n",
        "# Apply transformer\n",
        "df = buck.transform(df)\n",
        "# Inspect results\n",
        "df[['BATHSTOTAL','baths']].show(4)\n",
        "#---------------------------------------------------\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "# Plot distribution of sample_df\n",
        "sns.distplot(sample_df, axlabel='BEDROOMS')\n",
        "plt.show()\n",
        "\n",
        "# Create the bucket splits and bucketizer\n",
        "splits = [0,1, 2, 3, 4, 5, float('Inf')]\n",
        "buck = Bucketizer(splits=splits, inputCol= 'BEDROOOMS', outputCol= 'bedrooms')\n",
        "\n",
        "# Apply the transformation to df: df_bucket\n",
        "df_bucket = buck.transform(df)\n",
        "\n",
        "# Display results\n",
        "df_bucket[['BEDROOMS', 'bedrooms']].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu5xy20yZw7q"
      },
      "source": [
        "# One Hot Encoding\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "# Process\n",
        "  # Create Indexer transformer\n",
        "  stringIndexer = StringIndexer(inputCol = 'CITY', outputCol = 'City_Index')\n",
        "  # Fit transformer\n",
        "  model = stringIndexer.fit(df)\n",
        "  # Apply transformer\n",
        "  indexed = model.transform(df)\n",
        "\n",
        "  # Create Encoder Transformer\n",
        "  encoder = OneHotEncoder(inputCol = 'City_Index', outputCol = 'City_Vec')\n",
        "  # Apply the encoder transformer\n",
        "  encoded_df = encoder.transform(indexed)\n",
        "  # Inspect results\n",
        "  encoded_df[['City_Vec']].show(4)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "\n",
        "# Map strings to numbers with string indexer\n",
        "string_indexer = StringIndexer(inputCol='SCHOOLDISTRICTNUMBER', outputCol='School_Index')\n",
        "indexed_df = string_indexer.fit(df).transform(df)\n",
        "\n",
        "# Onehot encode indexed values\n",
        "encoder = OneHotEncoder(inputCol='School_Index', outputCol='School_Vec')\n",
        "encoded_df = encoder.transform(indexed_df)\n",
        "\n",
        "# Inspect the transformation steps\n",
        "encoded_df[['SCHOOLDISTRICTNUMBER', 'School_Index', 'School_Vec']].show(truncate=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsMsZy5jMr62"
      },
      "source": [
        "# **Building a Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGwDaDrISZYg"
      },
      "source": [
        "### **Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxGslQUMSnHd"
      },
      "source": [
        "\n",
        "\n",
        "1.   GeneralizedLinearRegression\n",
        "2.   IsotonicRegression\n",
        "3.   LinearRegression\n",
        "4.   DecisionTreeRegression\n",
        "5.   GBTRegression\n",
        "6.   RandomForestRegression\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A33oVvJOhDL5"
      },
      "source": [
        "**Train Test Splits for time Series**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "js-pfEyLMnqv"
      },
      "source": [
        "# Train and Test Splits for Time Series\n",
        "# Create Variables for max and min dates in our dataset\n",
        "max_date = df.agg({'OFFMKTDATE': 'max'}).collect()[0][0]\n",
        "min_date = df.agg({'OFFMKTDATE': 'min'}).collect()[0][0]\n",
        "\n",
        "# Find how many days our data spans\n",
        "from pyspark.sql.functions import datediff\n",
        "range_in_days = datediff(max_date, min_date)\n",
        "\n",
        "# Find the date to split the dataset on\n",
        "from pyspark.sql.functions import date_add\n",
        "split_in_days = round(range_in_days * 0.8)\n",
        "split_date = date_add(min_date, split_in_days)\n",
        "\n",
        "# Split the data into 80% train, 20% test\n",
        "train_df = df.where(df['OFFMKTDATE'] < split_date)\n",
        "test_df = df.where(df['OFFMKTDATE'] >= split_date)\\\n",
        "            .where(df['LISTDATE'] >= split_date) # to make sure that dates belong to the selected range\n",
        "# ------------------------------------------------\n",
        "def train_test_split_date(df, split_col, test_days= 45):\n",
        "  \"\"\"Calculate the date to split test and training sets\"\"\"\n",
        "  # Find how many days our data spans\n",
        "  max_date = df.agg({split_col: 'max'}).collect()[0][0]\n",
        "  min_date = df.agg({split_col: 'min'}).collect()[0][0]\n",
        "  # Subtract an integer number of days from the last date in dataset\n",
        "  split_date = max_date - timedelta(days= test_days)\n",
        "  return split_date\n",
        "\n",
        "# Find the date to use in spitting test and train\n",
        "split_date = train_test_split_date(df,'OFFMKTDATE')\n",
        "# Create Sequential Test and Training Sets\n",
        "train_df = df.where(df['OFFMKTDATE'] < split_date) \n",
        "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date) \n",
        "#----------------------------------------------------------------------------------\n",
        "from pyspark.sql.functions import datediff,to_date,lit\n",
        "\n",
        "split_date = to_date(lit('2017-12-10'))\n",
        "# Create Sequential Test set\n",
        "test_df = df.where(df['OFFMKTDATE'] >= split_date).where(df['LISTDATE'] <= split_date)\n",
        "\n",
        "# Create a copy of DAYSONMARKET to review later\n",
        "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
        "\n",
        "# Recalculate DAYSONMARKET from what we know on our split date\n",
        "test_df = test_df.withColumn('DAYSONMARKET', datediff(split_date,'LISTDATE'))\n",
        "\n",
        "# Review the difference\n",
        "test_df[['LISTDATE', 'OFFMKTDATE', 'DAYSONMARKET_Original', 'DAYSONMARKET']].show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhTfjVJwhL6w"
      },
      "source": [
        "**Preparing for Random Forest Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJyAwJ-QVu2"
      },
      "source": [
        "Pyspark Ml algorithms require that all the features are in a single column as a vector. We will need convert our columns for the Random Forest Regression do work. To continue it mention how does it do.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lXnB8JcRBe8"
      },
      "source": [
        "# Import the Vector Assembler transformation\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Replace missing values\n",
        "df = df.fillna(-1)\n",
        "\n",
        "# Define the columns to be converted to vectors\n",
        "features_cols = list(df.columns)\n",
        "\n",
        "# Remove the dependent variable from the list\n",
        "features_cols.remove('SALESCLOSEPRICE')\n",
        "\n",
        "# Create the vector assembler transformer\n",
        "vec = VectorAssembler(inputCols = features_cols, outputCol = 'features')\n",
        "\n",
        "# Apply the vector transformer to data\n",
        "df = vec.transform(df)\n",
        "\n",
        "# Select only the feature vectors and the dependent variable\n",
        "nl_ready_df = df.select(['SALESCLOSEPRICE','features'])\n",
        "\n",
        "# Inspect Results\n",
        "nl_ready_df.show(5)\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "obs_threshold = 30\n",
        "cols_to_remove = list()\n",
        "# Inspect first 10 binary columns in list\n",
        "for col in binary_cols[0:10]:\n",
        "  # Count the number of 1 values in the binary column\n",
        "  obs_count = df.agg({col: 'sum'}).collect()[0][0]\n",
        "  # If less than our observation threshold, remove\n",
        "  if obs_count <= obs_threshold:\n",
        "    cols_to_remove.append(col)\n",
        "    \n",
        "# Drop columns and print starting and ending dataframe shapes\n",
        "new_df = df.drop(*cols_to_remove)\n",
        "\n",
        "print('Rows: ' + str(df.count()) + ' Columns: ' + str(len(df.columns)))\n",
        "print('Rows: ' + str(new_df.count()) + ' Columns: ' + str(len(new_df.columns)))\n",
        "#----------------------------------------------------------------------------------\n",
        "# Replace missing values\n",
        "df = df.fillna(-1, subset=['WALKSCORE','BIKESCORE'])\n",
        "\n",
        "# Create list of StringIndexers using list comprehension\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\")\\\n",
        "            .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
        "# Create pipeline of indexers\n",
        "indexer_pipeline = Pipeline(stages=indexers)\n",
        "# Fit and Transform the pipeline to the original data\n",
        "df_indexed = indexer_pipeline.fit(df).transform(df)\n",
        "\n",
        "# Clean up redundant columns\n",
        "df_indexed = df_indexed.drop(*categorical_cols)\n",
        "# Inspect data transformations\n",
        "print(df_indexed.dtypes)\n",
        "#--------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFnQblPxpyIP"
      },
      "source": [
        "**Building a Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl6VigNfqLrx"
      },
      "source": [
        "Basic Model Parameters\n",
        "\n",
        "\n",
        "\n",
        "1.   featuresCol = \"features\" : Independent Variables\n",
        "2.   labelCol = \"label\": Dependent Variable\n",
        "1.   predictionCol = \"prediction\": Column where we will put our predictions\n",
        "2.   seed = None : Seed to the random process\n",
        "\n",
        "Process:\n",
        "\n",
        "\n",
        "1.   Import RandomForestRegressor\n",
        "2.   Set the RandomForestRegressor model\n",
        "1.   Fit the model to the dataframe\n",
        "2.   Make predictions with it.\n",
        "1.   See the results.\n",
        "2.   Evaluating the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukqR4WThpzR6"
      },
      "source": [
        "# Import RandomForestRegressor\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "# Initialize model with columns to utilize\n",
        "rf = RandomForestRegressor(featuresCol = 'features',\n",
        "                           labelCol ='SALESCLOSEPRICE',\n",
        "                           predictionCol = \"Prediction_Price\",\n",
        "                           seed = 42)\n",
        "\n",
        "# Train model\n",
        "model = rf.fit(train_df)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(test_df)\n",
        "\n",
        "# Inspect results\n",
        "predictions.select(\"Prediction_Price\",\"SALESCLOSEPRICE\").show(5)\n",
        "\n",
        "# Evaluating the model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Select Columns to compute test error\n",
        "evaluator = RegressionEvaluator(labelCol = 'SALESCLOSEPRICE',\n",
        "                                predictionCol = 'Prediction_Price')\n",
        "\n",
        "# Create evaluation metrics\n",
        "rmse = evaluator.evaluate(prediction, {evaluator.metricName: \"rmse\"})\n",
        "r2 = evaluator.evaluate(prediction, {evaluator.metricName: \"r2\"})\n",
        "\n",
        "# Print model metrics\n",
        "print('RMSE: ' + str(rmse))\n",
        "print('R**2: '- str(r2))\n",
        "\n",
        "#------------------------------------------------\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "# Train a Gradient Boosted Trees (GBT) model.\n",
        "gbt = GBTRegressor(featuresCol= 'features',\n",
        "                           labelCol='SALESCLOSEPRICE',\n",
        "                           predictionCol=\"Prediction_Price\",\n",
        "                           seed= 42\n",
        "                           )\n",
        "\n",
        "# Train model.\n",
        "model = gbt.fit(train_df)\n",
        "\n",
        "# Evaluating the model\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Select columns to compute test error\n",
        "evaluator = RegressionEvaluator(labelCol= 'SALESCLOSEPRICE', \n",
        "                                predictionCol = 'Prediction_Price')\n",
        "# Dictionary of model predictions to loop over\n",
        "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
        "for key, preds in models.items():\n",
        "    \n",
        "  # Create evaluation metrics\n",
        "  rmse = evaluator.evaluate(preds,{evaluator.metricName:\"rmse\"})\n",
        "  r2 = evaluator.evaluate(preds,{evaluator.metricName:\"r2\"})\n",
        "\n",
        "\n",
        "  # Print Model Metrics\n",
        "  print(key + ' RMSE: ' + str(rmse))\n",
        "  print(key + ' R^2: ' + str(r2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul1U--QCkMXg"
      },
      "source": [
        "Importance of the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RwlwZaAkPRZ"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert feature importances to a pandas column\n",
        "fi_df = pd.DataFrame(model.featureImportances.toArray(),\n",
        "                     columns = ['importance'])\n",
        "\n",
        "# Convert list of feature names to pandas column\n",
        "fi_df['feature'] = pd.Series(feature_cols)\n",
        "\n",
        "# Sort the data based on feature importance\n",
        "fi_df.sort_values(by = ['importance'], ascending = False, inplace = True)\n",
        "\n",
        "# Interpret results\n",
        "fi_df.head(9)\n",
        "\n",
        "#### Savings and loading the model\n",
        "\n",
        "# Save the model\n",
        "model.save('rfr_real_estate_model')\n",
        "\n",
        "# Load the model\n",
        "from pyspark.ml.regression import RandomForestRegressionModel\n",
        "model = RandomForestRegressionModel.load('rfr_real_estate_model')\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Select columns to compute test error\n",
        "evaluator = RegressionEvaluator(labelCol= 'SALESCLOSEPRICE', \n",
        "                                predictionCol = 'Prediction_Price')\n",
        "# Dictionary of model predictions to loop over\n",
        "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
        "for key, preds in models.items():\n",
        "    \n",
        "  # Create evaluation metrics\n",
        "  rmse = evaluator.evaluate(preds,{evaluator.metricName:\"rmse\"})\n",
        "  r2 = evaluator.evaluate(preds,{evaluator.metricName:\"r2\"})\n",
        "\n",
        "\n",
        "  # Print Model Metrics\n",
        "  print(key + ' RMSE: ' + str(rmse))\n",
        "  print(key + ' R^2: ' + str(r2))\n",
        "from pyspark.ml.regression import RandomForestRegressionModel\n",
        "\n",
        "\n",
        "# Save model\n",
        "model.save('rfr_no_listprice')\n",
        "\n",
        "# Load model\n",
        "loaded_model = RandomForestRegressionModel.load('rfr_no_listprice')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}