{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_Pyspark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOwxwTtiScZF+hOr7kqdK3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverRevilla/Spark_Pyspark/blob/main/Feature_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GlBtUqFYIh"
      },
      "source": [
        "## **Spark Changes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTbbFxIeFiaM"
      },
      "source": [
        "# return spark version\n",
        "spark.version\n",
        "\n",
        "# return python version\n",
        "import sys\n",
        "sys.version_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XCH3572GIdA"
      },
      "source": [
        "## **Verifying DataTypes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ8Vm7LSGLsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "b187cfc5-b224-4038-c07b-bb1d15e814db"
      },
      "source": [
        "# Verifying DataTypes\n",
        "\"\"\"\n",
        "# create list of actual dtypes to check\n",
        "actual_dtypes_list = df.dtypes\n",
        "print(actual_dtypes_list)\n",
        "\n",
        "# Iterate through the list of actual dtypes tuples\n",
        "for attribute_tuple in actual_dtypes_list:\n",
        "  \n",
        "  # Check if column name is dictionary of expected dtypes\n",
        "  col_name = attribute_tuple[0]\n",
        "  if col_name in validation_dict:\n",
        "\n",
        "    # Compare attribute types\n",
        "    col_type = attribute_tuple[1]\n",
        "    if col_type == validation_dict[col_name]:\n",
        "      print(col_name + ' has expected dtype.')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# create list of actual dtypes to check\\nactual_dtypes_list = df.dtypes\\nprint(actual_dtypes_list)\\n\\n# Iterate through the list of actual dtypes tuples\\nfor attribute_tuple in actual_dtypes_list:\\n  \\n  # Check if column name is dictionary of expected dtypes\\n  col_name = attribute_tuple[0]\\n  if col_name in validation_dict:\\n\\n    # Compare attribute types\\n    col_type = attribute_tuple[1]\\n    if col_type == validation_dict[col_name]:\\n      print(col_name + ' has expected dtype.')\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QGFUlDfFVRb"
      },
      "source": [
        "## **Visually Inspecting Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07joAU7jE5k7"
      },
      "source": [
        "# df.describe(['Name_column','Name_column2',...]).show()\n",
        "# pyspark.sql.functions.mean(col)\n",
        "# pyspark.sql.functions.skewness(col)  #lack of symmetry\n",
        "# pyspark.sql.functions.min(col)\n",
        "# cov(col1,col2)\n",
        "# corr(col1,col2)\n",
        "\n",
        "# pyspark.sql.functions.mean(col)\n",
        "df.agg({'SALESCLOSEPRICE':'mean'}).collect()\n",
        "\n",
        "# cov(col1,col2)\n",
        "df.cov('SALESCLOSEPRICE','YEARBUILT')\n",
        "\n",
        "# Sample PySpark DataFrames before converting to Pandas\n",
        "df.sample(False,0.5,42).count()\n",
        "sns.distplot(a)\n",
        "\n",
        "# Prepping for plotting a distribution\n",
        "import seaborn as sns\n",
        "sample_df = df.select(['SALESCLOSEPRICE']).sample(False,0.5,42) # 0.5 50% of the data, randomseed = 42\n",
        "pandas_df = sample_df.toPandas()\n",
        "sns.distplot(pandas_df)\n",
        "\n",
        "# Checking the maximun correlation value of all columns with 'SALESCLOSEPRICE'\n",
        "# Name and value of col with max corr\n",
        "corr_max = 0\n",
        "corr_max_col = columns[0]\n",
        "\n",
        "# Loop to check all columns contained in list\n",
        "for col in columns:\n",
        "    # Check the correlation of a pair of columns\n",
        "    corr_val = df.corr(col,'SALESCLOSEPRICE')\n",
        "    # Logic to compare corr_max with current corr_val\n",
        "    if corr_val > corr_max:\n",
        "        # Update the column name and corr value\n",
        "        corr_max = corr_val\n",
        "        corr_max_col = col\n",
        "\n",
        "print(corr_max_col)\n",
        "\n",
        "\n",
        "# Select a single column and sample and convert to pandas\n",
        "sample_df = df.select(['LISTPRICE']).sample(False, 0.5, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "# Plot distribution of pandas_df and display plot\n",
        "sns.distplot(pandas_df)\n",
        "plt.show()\n",
        "# Import skewness function\n",
        "from pyspark.sql.functions import skewness\n",
        "# Compute and print skewness of LISTPRICE\n",
        "print(df.agg({'LISTPRICE': 'skewness'}).collect())\n",
        "\n",
        "#Using Visualizations: Implot\n",
        "\n",
        "# Select a the relevant columns and sample\n",
        "sample_df = df.select(['SALESCLOSEPRICE', 'LIVINGAREA']).sample(False, 0.5, 42)\n",
        "\n",
        "# Convert to pandas dataframe\n",
        "pandas_df = sample_df.toPandas()\n",
        "\n",
        "# Linear model plot of pandas_df\n",
        "sns.lmplot(x= 'LIVINGAREA', y= 'SALESCLOSEPRICE', data= pandas_df)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bl-bySwpzc_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VvlZHXp1e6"
      },
      "source": [
        "# **Wrangling with Spark Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMPE3yMQp_eZ"
      },
      "source": [
        "**Dropping data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjLfPkhVp9OV"
      },
      "source": [
        "# List of columns to drop\n",
        "cols_to_drop = ['NO','UNITNUMBER','CLASS']\n",
        "# Drop columns\n",
        "df = df.drop(*cols_to_drop)\n",
        "\n",
        "# Show top 30 records\n",
        "df.show(30)\n",
        "\n",
        "# List of columns to remove from dataset\n",
        "cols_to_drop = ['STREETNUMBERNUMERIC','LOTSIZEDIMENSIONS']\n",
        "\n",
        "# Drop columns in list\n",
        "df = df.drop(*cols_to_drop)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZQC0ERRtZti"
      },
      "source": [
        "# Text filtering\n",
        "where(condicion)\n",
        "like(expression)\n",
        "isin(lista)\n",
        "df = df.where(df['POTENCIALSHORTSALE'].like('Not Disclosed'))\n",
        "\n",
        "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
        "df.select(['ASSUMABLEMORTGAGE']).distinct().show()\n",
        "\n",
        "# List of possible values containing 'yes'\n",
        "yes_values = ['Yes w/ Qualifying','Yes w/No Qualifying']\n",
        "\n",
        "# Filter the text values out of df but keep null values\n",
        "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
        "df = df.where(text_filter)\n",
        "\n",
        "# Print count of remaining records\n",
        "print(df.count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb269ueCtcMm"
      },
      "source": [
        "# Value filtering\n",
        "\n",
        "# Filtering outliers\n",
        "std_val = df.agg({'SALESCLOSEPRICE':'stddev'}).collect()[0][0]\n",
        "mean_val = df.agg({'SALESCLOSEPRICE':'mean'}).collect()[0][0]\n",
        "hi_bound = mean_val + (3*std_val)\n",
        "low_bound = mean_val - (3*std_val)\n",
        "df = df.where((df['LISTPRICE'] < hi_bound) & (df['LISTPRICE'] > low_bound))\n",
        "\n",
        "from pyspark.sql.functions import mean,stddev\n",
        "\n",
        "# Calculate values used for outlier filtering\n",
        "mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]\n",
        "stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]\n",
        "\n",
        "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
        "low_bound = mean_val - (3 * stddev_val)\n",
        "hi_bound = mean_val + (3 * stddev_val)\n",
        "\n",
        "# Filter the data to fit between the lower and upper bounds\n",
        "df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbz_KVgRtexj"
      },
      "source": [
        "# Dropping NA's or Nulls\n",
        "#.dropna(how = , thresh = ,subset =)\n",
        "DataFrame.dropna()\n",
        "# Drop any records with NULL Values\n",
        "df = df.dropna()\n",
        "# Drop records if both LISTPRICE and SALESPRICE are NULL\n",
        "df = df.dropna(how = 'all', subset = ['LISTPRICE','SALESCLOSEPRICE'])\n",
        "# Drop records where at least two columns have NULL values\n",
        "df = df.dropna(thresh = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHkn7m0ItheW"
      },
      "source": [
        "# Removing Duplicates\n",
        ".dropDuplicates()\n",
        "# Entire DataFrame\n",
        "df.dropDuplicates()\n",
        "# Check only a columns list\n",
        "df.dropDuplicates(['streetaddress'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wURuf-ix9lH"
      },
      "source": [
        "**Adjusting Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ0mWlIXyCm5"
      },
      "source": [
        "# Scaling data\n",
        "# Minmax scaling\n",
        "max_days = df.agg({'DAYSONMARKET':'max'}).collect()[0][0]\n",
        "min_days = df.agg({'DAYSONMARKET':'min'}).collect()[0][0]\n",
        "df = df.withColumn(\"scaled_days\",\n",
        "                   (df['DAYSONMARKET']- min_days)/(max_days - min_days))\n",
        "df[['scales_days']].show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEjyVR0G0PwA"
      },
      "source": [
        "# Standarization: Transform the data to standard normal distribution\n",
        "mean_days = df.agg({'DAYSONMARKET':'mean'}).collect()[0][0]\n",
        "stddev_days = df.agg({'DAYSONMARKET':'stddev'}).collect()[0][0]\n",
        "df = df.withColumn('ztrams_days',\n",
        "                   (df['DAYSONMARKET']- mean_days)/stddev_days)\n",
        "df.agg({'ztrans_days':'mean'}).collect()\n",
        "df.agg({'ztrans_days':'stddev'}).collect()\n",
        "\n",
        "# Define max and min values and collect them\n",
        "max_days = df.agg({'DAYSONMARKET': 'max'}).collect()[0][0]\n",
        "min_days = df.agg({'DAYSONMARKET': 'min'}).collect()[0][0]\n",
        "\n",
        "# Create a new column based off the scaled data\n",
        "df = df.withColumn('percentage_scaled_days', \n",
        "                  round((df['DAYSONMARKET'] - min_days) / (max_days - min_days)) * 100)\n",
        "\n",
        "# Calc max and min for new column\n",
        "print(df.agg({'percentage_scaled_days': 'max'}).collect())\n",
        "print(df.agg({'percentage_scaled_days': 'min'}).collect())\n",
        "\n",
        "## Function to scale\n",
        "def min_max_scaler(df, cols_to_scale):\n",
        "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
        "  for col in cols_to_scale:\n",
        "    # Define min and max values and collect them\n",
        "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
        "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
        "    new_column_name = 'scaled_' + col\n",
        "    # Create a new column based off the scaled data\n",
        "    df = df.withColumn(new_column_name, \n",
        "                      (df[col] - min_days) / (max_days - min_days))\n",
        "  return df\n",
        "  \n",
        "df = min_max_scaler(df, cols_to_scale)\n",
        "# Show that our data is now between 0 and 1\n",
        "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFIsAHIc0ROA"
      },
      "source": [
        "# Log scaling\n",
        "from pyspark.sql.functions import log\n",
        "df = df.withColumn('log_SalesClosePrice',log(df['SALESCLOSEPRICE']))\n",
        "\n",
        "from pyspark.sql.functions import log\n",
        "\n",
        "# Compute the skewness\n",
        "print(df.agg({'YEARBUILT': 'skewness'}).collect())\n",
        "\n",
        "# Calculate the max year\n",
        "max_year = df.agg({'YEARBUILT': 'max'}).collect()[0][0]\n",
        "\n",
        "# Create a new column of reflected data\n",
        "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
        "\n",
        "# Create a new column based reflected data\n",
        "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZZEpjHBKEPI"
      },
      "source": [
        "**Working with missing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI7rOkfbKI27"
      },
      "source": [
        "# Assessing Missing Values\n",
        "# isNull()\n",
        "df.where(df['ROOF'].isNull()).count()\n",
        "\n",
        "# Function to automate dropping\n",
        "def column_dropper(df, threshold):\n",
        "  # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
        "  total_records = df.count()\n",
        "  for col in df.columns:\n",
        "    # Calculate the percentage of missing values\n",
        "    missing = df.where(df[col].isNull()).count()\n",
        "    missing_percent = missing / total_records\n",
        "    # Drop column if percent of missing is more than threshold\n",
        "    if missing_percent > threshold:\n",
        "      df = df.drop(col)\n",
        "  return df\n",
        "\n",
        "# Drop columns that are more than 60% missing\n",
        "df = column_dropper(df, 0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1bXAMATNSD-"
      },
      "source": [
        "# Plotting Missing Values\n",
        "import seaborn as sns\n",
        "sub_df = df.select(['ROOMAREA1'])\n",
        "sample_df = sub_df.sample(False,0.5,4)\n",
        "pandas_df = sample_df.toPandas()\n",
        "sns.heatmap(data = pandas.isnull())\n",
        "\n",
        "# Sample the dataframe and convert to Pandas\n",
        "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "\n",
        "# Convert all values to T/F\n",
        "tf_df = pandas_df.isnull()\n",
        "\n",
        "# Plot it\n",
        "sns.heatmap(data=tf_df)\n",
        "plt.xticks(rotation=30, fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# Set the answer to the column with the most missing data\n",
        "answer = 'BACKONMARKETDATE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2T_IqqrNTy8"
      },
      "source": [
        "# Imputation of missing values\n",
        "# fillna(value, subset = None): \n",
        "#value: the value to replace missing with\n",
        "#subset: the list of column names to replace missing\n",
        "# Replacing missing values with zero\n",
        "df.fillna(0, subset = ['DAYSONMARKET'])\n",
        "\n",
        "# Replacing with the mean value for that column\n",
        "col_mean = df.agg({'DAYSONMARKET':'mean'}).collect()[0][0]\n",
        "df.fillna(col_mean,subset = ['DAYSONMARKET'])\n",
        "\n",
        "# Sample the dataframe and convert to Pandas\n",
        "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
        "pandas_df = sample_df.toPandas()\n",
        "\n",
        "# Convert all values to T/F\n",
        "tf_df = pandas_df.isnull()\n",
        "\n",
        "# Plot it\n",
        "sns.heatmap(data=tf_df)\n",
        "plt.xticks(rotation=30, fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.show()\n",
        "\n",
        "# Set the answer to the column with the most missing data\n",
        "answer = 'BACKONMARKETDATE'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMpexXfWT1eO"
      },
      "source": [
        "**Joins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaU0ZoZ4T4mi"
      },
      "source": [
        "# Pyspark DataFrame Joins\n",
        "DataFrame.join(\n",
        "    other, # Other DataFrame to merge\n",
        "    on = None, # The keys to join on\n",
        "    how = None # Type of join to perform (default is 'inner')\n",
        ")\n",
        "\n",
        "# Example:\n",
        "hdf.show(2)\n",
        "# Specify the condition\n",
        "cond = [df['OFFMARKETDATE'] == hdf['dt']]\n",
        "# Join two hdf onto df\n",
        "df = df.join(hdf, on = cond, 'left')\n",
        "# How many sales occurred on bank holidays\n",
        "df.where(df['nm'].isNull()).count()\n",
        "\n",
        "# Cast data types\n",
        "walk_df = walk_df.withColumn('longitude', walk_df['longitude'].cast('double'))\n",
        "walk_df = walk_df.withColumn('latitude', walk_df['latitude'].cast('double'))\n",
        "\n",
        "# Round precision\n",
        "df = df.withColumn('longitude', round('longitude', 5))\n",
        "df = df.withColumn('latitude', round('latitude', 5))\n",
        "\n",
        "# Create join condition\n",
        "condition = [walk_df['latitude'] == df['latitude'], walk_df['longitude'] == df['longitude']]\n",
        "\n",
        "# Join the dataframes together\n",
        "join_df = df.join(walk_df, on=condition, how='left')\n",
        "# Count non-null records from new field\n",
        "print(join_df.where(~join_df['walkscore'].isNull()).count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlOyH0QNV73E"
      },
      "source": [
        "# Spak SQL Join\n",
        "# Register the dataframe as a temp table\n",
        "df.createOrReplaceTempView(\"df\")\n",
        "hdf.createOrReplaceTempView('hdf')\n",
        "# Write a SQL statement\n",
        "sql_df = spark.sql(\"\"\"\n",
        "    SELECT * \n",
        "    FROM df\n",
        "    LEFT JOIN hdf\n",
        "    ON df.OFFMARKETDATE =  hdf.dt \n",
        "\"\"\")\n",
        "\n",
        "# Register dataframes as tables\n",
        "df.createOrReplaceTempView('df')\n",
        "walk_df.createOrReplaceTempView('walk_df')\n",
        "\n",
        "# SQL to join dataframes\n",
        "join_sql = \t\"\"\"\n",
        "\t\t\tSELECT \n",
        "\t\t\t\t*\n",
        "\t\t\tFROM df\n",
        "\t\t\tLEFT JOIN walk_df\n",
        "\t\t\tON df.longitude = walk_df.longitude\n",
        "\t\t\tAND df.latitude = walk_df.latitude\n",
        "\t\t\t\"\"\"\n",
        "# Perform sql join\n",
        "joined_df = spark.sql(join_sql)\n",
        "\n",
        "# Join on mismatched keys precision \n",
        "wrong_prec_cond = [walk_df['longitude'] == df_orig['longitude'], walk_df['latitude'] == df_orig['latitude']]\n",
        "wrong_prec_df = df_orig.join(walk_df, on= wrong_prec_cond, how='left')\n",
        "\n",
        "# Compare bad join to the correct one\n",
        "print(wrong_prec_df.where(wrong_prec_df['walkscore'].isNull()).count())\n",
        "print(correct_join_df.where(correct_join_df['walkscore'].isNull()).count())\n",
        "\n",
        "# Create a join on too few keys\n",
        "few_keys_cond = [df['longitude'] == walk_df['longitude']]\n",
        "few_keys_df = df.join(walk_df, on=few_keys_cond, how='left')\n",
        "\n",
        "# Compare bad join to the correct one\n",
        "print(\"Record Count of the Too Few Keys Join Example: \" + str(few_keys_df.count()))\n",
        "print(\"Record Count of the Correct Join Example: \" + str(correct_join_df.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq9FOTS7bMp8"
      },
      "source": [
        "# **Feature generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn9cynHNbR5F"
      },
      "source": [
        "# Creating a new feature area, by multiplicating\n",
        "df = df.column('TSQTF', (df['WIDTH']*df['LENGTH']))\n",
        "\n",
        "# Diference two date columns\n",
        "df = df.withColumn('DAYSONMARKET', datediff('OFFMARKETDATE','LISTDATE'))\n",
        "\n",
        "# Check the python libraries FeatureTools and TSFresh\n",
        "\n",
        "# ---- Exercises -----------------------------------------------------------------------------------------\n",
        "# Lot size in square feet\n",
        "acres_to_sqfeet = 43560\n",
        "df = df.withColumn('LOT_SIZE_SQFT', df['ACRES']*acres_to_sqfeet)\n",
        "\n",
        "# Create a new column YARD_SIZE\n",
        "df = df.withColumn('YARD_SIZE',df['LOT_SIZE_SQFT'] - df['FOUNDATIONSIZE'])\n",
        "\n",
        "# Corr of ACRES vs SALESCLOSEPRICE\n",
        "print(\"Corr of ACRES vs SALESCLOSEPRICE: \" + str(df.corr('ACRES','SALESCLOSEPRICE')))\n",
        "\n",
        "# Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE\n",
        "print(\"Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE: \" + str(df.corr('FOUNDATIONSIZE', 'SALESCLOSEPRICE')))\n",
        "\n",
        "# Corr of YARD_SIZE vs SALESCLOSEPRICE\n",
        "print('Corr of YARD_SIZE vs SALESCLOSEPRICE: ' + str(df.corr('YARD_SIZE','SALESCLOSEPRICE')))\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# ASSESSED_TO_LIST\n",
        "df = df.withColumn('ASSESSED_TO_LIST', (df['ASSESSEDVALUATION']/df['LISTPRICE']))\n",
        "df[['ASSESSEDVALUATION', 'LISTPRICE', 'ASSESSED_TO_LIST']].show(5)\n",
        "# TAX_TO_LIST\n",
        "df = df.withColumn('TAX_TO_LIST',(df['TAXES']/df['LISTPRICE']))\n",
        "df[['TAX_TO_LIST', 'TAXES', 'LISTPRICE']].show(5)\n",
        "# BED_TO_BATHS\n",
        "df = df.withColumn('BED_TO_BATHS',(df['BEDROOMS']/df['BATHSTOTAL']))\n",
        "df[['BED_TO_BATHS', 'BEDROOMS', 'BATHSTOTAL']].show(5)\n",
        "\n",
        "# --------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYgwBjB9OhfN"
      },
      "source": [
        "Time features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_6itvcaOkmp"
      },
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Cast the data type to Date\n",
        "# to_date()\n",
        "df = df.withColumn('LISTDATE', to_date('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import year_month\n",
        "# Create a new column of year number\n",
        "df = df.withColumn('LIST_YEAR',year('LISTDATE'))\n",
        "\n",
        "# Create a new column of month number\n",
        "df = df.withColumn('LIST_MONTH', month('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import dayofmonth, weekofyear\n",
        "\n",
        "# Create new columns of the day number within the month\n",
        "df = df.withColumns('LIST_DAYOFMONTH', dayofmonth('LISTDATE'))\n",
        "\n",
        "# Create new columns of the week number within the year\n",
        "df = df.withColumn('LIST_WEEKOFYEAR', weekofyear('LISTDATE'))\n",
        "\n",
        "from pyspark.sql.functions import datediff\n",
        "\n",
        "# Calculate difference between two date fields\n",
        "df.withColumn('DAYSONMARKET', datediff('OFFMARKETDATE','LISTDATE'))\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import lag\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Create Window\n",
        "w = Window().orderBy(m_df['DATE'])\n",
        "# Create lagged column\n",
        "m_df = m_df.withColumn('MORTAGE-1wk', lag('MORTAGE', count = 1).over(w))\n",
        "\n",
        "# ----- Exercises --------------------\n",
        "# Import needed fucntions\n",
        "from pyspark.sql.functions import to_date,dayofweek\n",
        "\n",
        "# Convert to date type\n",
        "df = df.withColumn('LISTDATE',to_date('LISTDATE'))\n",
        "\n",
        "# Get the day of the week\n",
        "df = df.withColumn('List_Day_of_Week',dayofweek('LISTDATE'))\n",
        "\n",
        "# Sample and convert to pandas dataframe\n",
        "sample_df = df.sample(False,0.5,42).toPandas()\n",
        "\n",
        "# Plot caount plot of of day of week\n",
        "sns.countplot(x='List_Day_of_Week', data = sample_df )\n",
        "plt.show()\n",
        "#--------------------------------------\n",
        "from pyspark.sql.functions import year\n",
        "\n",
        "# Initialize dataframes\n",
        "df = real_estate_df\n",
        "price_df = median_prices_df\n",
        "\n",
        "# Create year column\n",
        "df = df.withColumn('list_year', year('LISTDATE'))\n",
        "\n",
        "# Adjust year to match\n",
        "df = df.withColumn('report_year',(df['list_year'] - 1))\n",
        "\n",
        "# Create join condition\n",
        "condition = [df['CITY'] == price_df['City'], df['report_year'] == price_df['Year']]\n",
        "\n",
        "# Join the dataframes together\n",
        "df = df.join(price_df, on=condition, how= 'left')\n",
        "# Inspect that new columns are available\n",
        "df[['MedianHomeValue']].show()\n",
        "\n",
        "# -----------------------------------------\n",
        "from pyspark.sql.functions import lag, datediff, to_date\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Cast data type\n",
        "mort_df = mort_df.withColumn('DATE', to_date('DATE'))\n",
        "\n",
        "# Create window\n",
        "w = Window().orderBy(mort_df['DATE'])\n",
        "# Create lag column\n",
        "mort_df = mort_df.withColumn('DATE-1', lag('DATE', count=1).over(w))\n",
        "\n",
        "# Calculate difference between date columns\n",
        "mort_df = mort_df.withColumn('Days_Between_Report', datediff('DATE', 'DATE-1'))\n",
        "# Print results\n",
        "mort_df.select('Days_Between_Report').distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}