{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigDataFundamentals_Spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQPoFKxlUZpeDEX+axV2YE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverRevilla/Spark_Pyspark/blob/main/BigDataFundamentals_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNlzLOrtIaSu"
      },
      "source": [
        "# SparkContext\n",
        "# SparkSession\n",
        "\n",
        "\"\"\"\n",
        "# Print the version of spark\n",
        "from pyspark import SparkContext as sc\n",
        "sc.version\n",
        "\n",
        "# Print the version of Python\n",
        "sc.pythonVer\n",
        "\n",
        "# Print the name of the compute here it is running\n",
        "sc.master\n",
        "\n",
        "\"\"\"\n",
        "# Leyendo listas de python en spark\n",
        "# Se convierte la lista en un RDD\n",
        "\"\"\"\n",
        "# Create a Python list of numbers from 1 to 100 \n",
        "numb = range(1, 100)\n",
        "\n",
        "# Load the list into PySpark  \n",
        "spark_data = sc.parallelize(numb)\n",
        "\"\"\"\n",
        "\n",
        "# Load a local file into PySpark shell\n",
        "\n",
        "\"\"\"\n",
        "Suponiendo que se tiene un file_path\n",
        "\n",
        "lines = sc.textFile(file_path)\n",
        "\"\"\"\n",
        "\n",
        "# Funciones map() and filter()\n",
        "# La sintaxis es map(funcion, lista) similar para filter\n",
        "# lambda x: expresion\n",
        "# para llamar a map) o a filter() usar list.\n",
        "'''\n",
        "# Print my_list in the console\n",
        "print(\"Input list is\", my_list)\n",
        "\n",
        "# Square all numbers in my_list\n",
        "squared_list_lambda = list(map(lambda x: x**2,my_list ))\n",
        "\n",
        "\n",
        "# Print the result of the map function\n",
        "print(\"The squared numbers are\", squared_list_lambda)\n",
        "\n",
        "# Print my_list2 in the console\n",
        "print(\"Input list is:\", my_list2)\n",
        "\n",
        "# Filter numbers divisible by 10\n",
        "filtered_list = list(filter(lambda x: x%10 == 0, my_list2))\n",
        "\n",
        "# Print the numbers divisible by 10\n",
        "print(\"Numbers divisible by 10 are:\", filtered_list)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy0-yztsSQUZ"
      },
      "source": [
        "Indtroducción a PySpark RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0odN5sY-SKL8"
      },
      "source": [
        "# Concepto: Resilient Distributed Datasets\n",
        "# Se distribuye en varios nodos.\n",
        "# Características: \n",
        "#   Resilient: Resistente a fallas\n",
        "#   Distributed: Distribuido a través de múltiples equipos\n",
        "#   Datasets:  Collección de particiones de datos\n",
        "\n",
        "\"\"\"\n",
        "Para crear RDDs\n",
        "\n",
        "# Desde una lista\n",
        "parallelize()\n",
        "numRDD = sc.parallelize([1,2,3,4])\n",
        "helloRDD = sc.parallelize(\"Hello world\")\n",
        "\n",
        "# Desde un archivo externo\n",
        "fileRDD = sc.textFile(filepath)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Partitions\n",
        "# getNumPartitions()\n",
        "\"\"\"\n",
        "numRDD = sc.parallelize(range(10), minPartitions = 6)\n",
        "\n",
        "fileRDD = sc.textFile(\"README.md\",minPartitions = 6)\n",
        "\n",
        "\"\"\"\n",
        "# map() and filter() para RDDs\n",
        "# Se necesita aplicar collect() method para reunir todos los elementos del RDD\n",
        "\"\"\"\n",
        "# Create map() transformation to cube numbers\n",
        "cubedRDD = numbRDD.map(lambda x: x**3)\n",
        "\n",
        "# Collect the results\n",
        "numbers_all = cubedRDD.collect()\n",
        "\n",
        "# Print the numbers from numbers_all\n",
        "for numb in numbers_all:\n",
        "\tprint(numb)\n",
        "\n",
        "\"\"\"\n",
        "# Filtrado de RDDs\n",
        "# lambda line: \"Palabra\" in line.split()\n",
        "# .count()\n",
        "# take()    toma un numero de elementos del RDD\n",
        "\"\"\"\n",
        "# Filter the fileRDD to select lines with Spark Keyword\n",
        "fileRDD_filter = fileRDD.filter(lambda line: \"Spark\" in line.split())\n",
        "\n",
        "# How many lines are there in fileRDD\n",
        "print(\"The total number of lines with the keyword Spark is\",\n",
        "fileRDD_filter.count())\n",
        "\n",
        "# Print the first four lines of fileRDD\n",
        "for line in fileRDD_filter.take(4):\n",
        "  print(line)\n",
        "\"\"\"\n",
        "\n",
        "# Trabajando con Keys y Values\n",
        "\n",
        "\"\"\" Teoría\n",
        "my_tuple = [('Sam',23),('Mary', 34),('Peter',25)]\n",
        "pairRDD_tuple = sc.parallelize(my_tuple)\n",
        "\n",
        "my_list = ['Sam 23','Mary 34','Peter 25']\n",
        "regularRDD = sc.parallelize(my_list)\n",
        "pairRDD_RDD = regularRDD.map(lambda s: (s.split('')[0],s.split('')[1]))\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Transformaciones con keys y values\n",
        "\n",
        "reduceByKey(func): Combina los valores con el mismo key\n",
        "groupByKey(): Agrupa valores con el mismo key\n",
        "sorByKey(): Retorna un RDD ordenado por key\n",
        "join(): Junta dos RDDs de la forma key,values por su key\n",
        "\n",
        "Ejemplos:\n",
        "---------------------------------------------------------------------------------------\n",
        "regularRDD = sc.parallelize([(\"Messi\",23),(\"Ronaldo\",34),(\"Neymar\",22),(\"Messi\",24)])\n",
        "pairRDD_reducebykey = regularRDD.reduceByKey(lambda x,y : x + y)\n",
        "pairRDD_reducebykey.collect()\n",
        "\n",
        "# Create PairRDD Rdd with key value pairs\n",
        "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
        "# Apply reduceByKey() operation on Rdd\n",
        "Rdd_Reduced = Rdd.reduceByKey(lambda x, y : x + y)\n",
        "# Iterate over the result and print the output\n",
        "for num in Rdd_Reduced.collect():\n",
        "  print(\"Key {} has {} Counts\".format(num[0], num[1]))\n",
        "---------------------------------------------------------------------------------------\n",
        "\n",
        "pairRDD_reducebykey_rev = pairRDD_reducebykey.map(lambda x: (x[1], x[0]))\n",
        "pairRDD_reducebykey_rev.sortByKey(ascending = False).collect()\n",
        "\n",
        "----------------------------------------------------------------------------------------\n",
        "airports = [('US','JFK),('UK','LHR'),('FR','CDG'),('US','SFO')]\n",
        "regularRDD = sc.parallelize(airports)\n",
        "pairRDD_group = regularRDD.groupByKey().collect()\n",
        "for cont, air in pairRDD_group:\n",
        "  print(cont,list(air))\n",
        "\n",
        "----------------------------------------------------------------------------------------\n",
        "RDD1 = sc.parallelize([(\"Messi\",34),(\"Ronaldo\",32),(\"Neymar\",24)])\n",
        "RDD2 = sc.parallelize([(\"Ronaldo\",80),(\"Neymar\",120),(\"Messi\",100)])\n",
        "\n",
        "RDD1.join(RDD2).collect()\n",
        "\n",
        "# Otras transformaciones\n",
        "# En este caso para RDD normales\n",
        "x = [1,2,3,4]\n",
        "RDD = sc.parallelize(x)\n",
        "RDD.reduce(lambda x,y: x + y) --- suma todos los elelementos\n",
        "\n",
        "saveAsTextFile()\n",
        "RDD.saveAsTextFile(\"tempFile\") separa particion por particion\n",
        "\n",
        "coalesce()\n",
        "RDD.coalesce(1).saveAsTextFile(\"tempFile\") separa como un unico textfile\n",
        "\n",
        "\n",
        "## For pair RDDs: Son acciones\n",
        "countByKey(): cuenta valores por key\n",
        "collectAsMap(): traslada los valores como si fuese un diccionario\n",
        "\n",
        "rdd = sc.parallelize([(\"a\",1),(\"b\",1),(\"a\",1)])\n",
        "for kee, val in rdd.countByKey().items():  #Notar que se usa .item() para iterar\n",
        "  print(kee,val)\n",
        "\n",
        "sc.parallelize([(1,2),(3,4)]).collectAsMap()\n",
        "\"\"\"\n",
        "# Creando y transformando un RDD\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Create a baseRDD from the file path\n",
        "baseRDD = sc.textFile(file_path)\n",
        "\n",
        "# Split the lines of baseRDD into words\n",
        "splitRDD = baseRDD.flatMap(lambda x: x.split())\n",
        "\n",
        "# Count the total numbers of words\n",
        "print(\"Total number of words in splitRDD:\", splitRDD.count())\n",
        "\n",
        "# Convert the words in lower case and remove stopo ords from the stop_words curated list\n",
        "\n",
        "splitRDD_no_stop = splitRDD.filter(lambda x: x.lower() not in stop_words)\n",
        "\n",
        "# Create a tuple of the word and one\n",
        "splitRDD_no_stop_words = splitRDD_no_stop.map(lambda w: (w, 1))\n",
        "\n",
        "# Count of the number of occurences of each word\n",
        "resultRDD = splitRDD_no_stop_words.reduceByKey(lambda x,y: x + y)\n",
        "\n",
        "# Display the first 10 words and their frequencies from the input RDD\n",
        "for word in resultRDD.take(10):\n",
        "\tprint(word)\n",
        "\n",
        "# Swap the keys and values from the input RDD\n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "# Sort the keys in descending order\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending = False)\n",
        "\n",
        "# Show the top 10 most frequent ords and their frequencies form the sorted RDD\n",
        "for word in resultRDD_swap_sort.take(10):\n",
        "\tprint(\"{},{}\".format(word[1],word[0]))\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JUfC0gYrdO_"
      },
      "source": [
        "Pyspark SQL & DataFrames"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHItRMrPrbOY"
      },
      "source": [
        "# Se pueden crerar dataframes de dos formas:\n",
        "# Desde un RDD\n",
        "# Leyendo un archivo\n",
        "\"\"\"\n",
        "Primera forma\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "iphonesRDD = sc.parallelize([lista])\n",
        "names = [lista]\n",
        "\n",
        "iphone_dataframe = spark.createDataFrame(iphonesRDD, schema = names)\n",
        "\n",
        "# Create an RDD from the list\n",
        "rdd = sc.parallelize(sample_list)\n",
        "\n",
        "# Create a Pyspark DataFrame\n",
        "names_df = spark.createDataFrame(rdd, schema = ['Name', 'Age'])\n",
        "\n",
        "# Check the type of names_df\n",
        "print(\"The type of names_df is\", type(names_df))\n",
        "\n",
        "El tipo de objeto será un pyspark.sql.dataframe\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "Segunda forma\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "df_csv = spark.read.csv(\"people.csv\", header = True, inferSchema = True)\n",
        "df_json = spark.read.json(\"people.json\", header = True, inferSchema = True)\n",
        "df_txt = spark.read.txt(\"people.txt\",header = True, inferSchema = True)\n",
        "\n",
        "# Create an DataFrame from file_path\n",
        "people_df = spark.read.csv(file_path, header = True, inferSchema = True)\n",
        "\n",
        "# Check the type of people_df\n",
        "print(\"The type of people_df is\", type(people_df))\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0atahRdsOT0"
      },
      "source": [
        "DataFrame operator in PySpark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vXfqpk_sReB"
      },
      "source": [
        "# DataFrame Transformations:\n",
        "# select(), filter(),groupby(),orderby(),dropDuplicates(),withColumnRenamed()\n",
        "\"\"\"\n",
        "df_id_age = test.select(\"Age\")\n",
        "df_id_age.show(3)\n",
        "\n",
        "nuevodataframe = dataframe.filter(dataframe.columna > parametro)\n",
        "nuevodataframe.show(3)\n",
        "\n",
        "nuevodataframe = dataframe.groupby('edad')\n",
        "nuevodataframe.count().show(3)\n",
        "\n",
        "nuevodataframe = dataframe.orderBy('edad')\n",
        "nuevodataframe.show(3)\n",
        "\n",
        "df_id_age = test.select(\"Age\").dropDuplicates()\n",
        "df_id_age.count()\n",
        "\n",
        "nuevodataframe = dataframe.withColumnRenamed('edad',\"Edad\")\n",
        "nuevodataframe.show(3)\n",
        "\n",
        "\"\"\"\n",
        "# DataFrame actions:\n",
        "# head(),show(),count(),columns(), describe()\n",
        "\"\"\"\n",
        "dataframe.printSchema()\n",
        "dataframe.columns\n",
        "dataframe.describe().show()\n",
        "\n",
        "# Print the first 10 observations\n",
        "people_df.show(10)\n",
        "\n",
        "# Count the number of rows\n",
        "print(\"There are {} rows in the people_df DataFrame.\".format(people_df.count()))\n",
        "\n",
        "# Count the numer of colums and their names\n",
        "print(\"There are {} columns in the people_df DataFrame and their names are {}\".format(len(people_df.columns), people_df.columns))\n",
        "\n",
        "  # Select name, sex and date of birth columns\n",
        "  people_df_sub = people_df.select('name','sex','date of birth')\n",
        "\n",
        "  # Print the first 10 observations from people_df_sub\n",
        "  people_df_sub.show(10)\n",
        "\n",
        "  # Remove duplicate entries from people_df_sub\n",
        "  peoplpe_df_sub_nodup = people_df_sub.dropDuplicates()\n",
        "\n",
        "  # Count the number of rows\n",
        "  print(\"There were {} rows before removing duplicates, and {} rows after removing duplicates\".format(people_df_sub.count(),peoplpe_df_sub_nodup.count()))\n",
        "\n",
        "# Filter people_df to select females \n",
        "people_df_female = people_df.filter(people_df.sex == \"female\")\n",
        "\n",
        "# Filter people_df to select males\n",
        "people_df_male = people_df.filter(people_df.sex == \"male\")\n",
        "\n",
        "# Count the number of rows\n",
        "print(\"There are {} rows in the people_df_female DataFrame and {} rows in the people_df_male DataFrame\".format(people_df_female.count(), people_df_male.count()))\n",
        "\n",
        "\"\"\"\n",
        "### Interacting with DataFrames using PySpark SQL\n",
        "\"\"\"\n",
        "# Create a temporary table \"people\"\n",
        "people_df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Construct a query to select the names of the people from the temprary table \"people\"\n",
        "query = '''SELECT name FROM people'''\n",
        "\n",
        "# Assign the result of Spark's query to people_df_names\n",
        "people_df_names = spark.sql(query)\n",
        "\n",
        "# Print the top 10 names of the people\n",
        "people_df_names.show(10)\n",
        "\n",
        "# Filter the people table to select females sex\n",
        "people_female_df = spark.sql('SELECT * FROM people WHERE sex == \"female\"')\n",
        "\n",
        "# Filter the people table DataFrame to select male sex\n",
        "people_male_df = spark.sql('SELECT * FROM people WHERE sex == \"male\"')\n",
        "\n",
        "# Count the number of rows in both DataFrames\n",
        "print(\"There are {} rows in the people_female_df and {} rows in the people_male_df DataFrames\".format(people_female_df.count(),people_male_df.count()))\n",
        "\n",
        "\"\"\"\n",
        "# Data visualization\n",
        "\"\"\"\n",
        "# Check the column names of names_df\n",
        "print(\"The column names of names_df are\", names_df.columns)\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df_pandas = names_df.toPandas()\n",
        "\n",
        "# Create a horizontal bar plot\n",
        "df_pandas.plot(kind = \"barh\", x = 'Name', y = 'Age', colormap = 'winter_r')\n",
        "plt.show()\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Load the Dataframe\n",
        "fifa_df = spark.read.csv(file_path, header = True, inferSchema= True)\n",
        "\n",
        "# Check the schema of columns\n",
        "fifa_df.printSchema()\n",
        "\n",
        "# Show the first 10 observations\n",
        "fifa_df.show(10)\n",
        "\n",
        "# Print the total number of rows\n",
        "print(\"There are {} rows in the fifa_df DataFrame\".format(fifa_df.count()))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Create a temporary view of fifa_df\n",
        "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
        "\n",
        "# Construct the \"query\"\n",
        "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
        "\n",
        "# Apply the SQL \"query\"\n",
        "fifa_df_germany_age = spark.sql(query)\n",
        "\n",
        "# Generate basic statistics\n",
        "fifa_df_germany_age.describe().show()\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Convert fifa_df to fifa_df_germany_age_pandas DataFrame\n",
        "fifa_df_germany_age_pandas = fifa_df_germany_age.toPandas()\n",
        "\n",
        "# Plot the 'Age' density of Germany Players\n",
        "fifa_df_germany_age_pandas.plot(kind = 'density')\n",
        "plt.show()\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilszZJ7IatGD"
      },
      "source": [
        "MACHINE LEARNING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBR67oFQqcK1"
      },
      "source": [
        "Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9FsjAEXauhM"
      },
      "source": [
        "\"\"\"\n",
        "Import pyspark.mllib recommendation submodule and Alternating Least Squares class.\n",
        "\n",
        "Import pyspark.mllib recommendation submodule and Alternating Least Squares class.\n",
        "Import pyspark.mllib classification submodule and Logistic Regression with LBFGS class.\n",
        "Import pyspark.mllib clustering submodule and kmeans class.\n",
        "\n",
        "# Import the library for ALS\n",
        "from pyspark.mllib.recommendation import ALS\n",
        "\n",
        "# Import the library for Logistic Regression\n",
        "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
        "\n",
        "# Import the library for Kmeans\n",
        "from pyspark.mllib.clustering import KMeans\n",
        "\"\"\"\n",
        "\n",
        "# Collaborative filtering : Recommendation Algorithm\n",
        "\"\"\"\n",
        "# Load the data into RDD\n",
        "data = sc.textFile(file_path)\n",
        "\n",
        "# Split the RDD \n",
        "ratings = data.map(lambda l: l.split(','))\n",
        "\n",
        "# Transform the ratings RDD \n",
        "ratings_final = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])))\n",
        "\n",
        "# Split the data into training and test\n",
        "training_data, test_data = ratings_final.randomSplit([0.8,0.2])\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Create the ALS model on the training data\n",
        "model = ALS.train(training_data, rank=10, iterations=10)\n",
        "\n",
        "# Drop the ratings column \n",
        "testdata_no_rating = test_data.map(lambda p: (p[0],p[1]))\n",
        "\n",
        "# Predict the model  \n",
        "predictions = model.predictAll(testdata_no_rating)\n",
        "\n",
        "# Return the first 2 rows of the RDD\n",
        "predictions.take(2)\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Prepare ratings data\n",
        "rates = ratings_final.map(lambda r: ((r[0],r[1]),r[2]))\n",
        "\n",
        "# Prepare predictions data\n",
        "preds = predictions.map(lambda r: ((r[0],r[1]),r[2]))\n",
        "\n",
        "# Join the ratings data with predictions data\n",
        "rates_and_preds = rates.join(preds)\n",
        "\n",
        "# Calculate and print MSE\n",
        "MSE = rates_and_preds.map(lambda r: (r[1][0]-r[1][1])**2).mean()\n",
        "print(\"Mean Squared Error of the model for the test data = {:.2f} \".format(MSE))\n",
        "\n",
        "rates_and_preds.take(5)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW5rL64qqfFa"
      },
      "source": [
        "Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IKZumU5qgWH"
      },
      "source": [
        "# Load the datasets into RDDs\n",
        "\"\"\"\n",
        "spam_rdd = sc.textFile(file_path_spam)\n",
        "non_spam_rdd = sc.textFile(file_path_non_spam)\n",
        "\"\"\"\n",
        "# Split the email messages into words\n",
        "\"\"\"\n",
        "spam_words = spam_rdd.flatMap(lambda email: email.split())\n",
        "non_spam_words = non_spam_rdd.flatMap(lambda email: email.split())\n",
        "\"\"\"\n",
        "# Print the first element in the split RDD\n",
        "\"\"\"\n",
        "print(\"The first element in spam_words is\", spam_words.first())\n",
        "print(\"The first element in non_spam_words is\",non_spam_words.first())\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Create a HashingTf instance with 200 features\n",
        "tf = HashingTF(numFeatures = 200)\n",
        "\n",
        "# Map each word to one feature\n",
        "spam_features = tf.transform(spam_words)\n",
        "non_spam_features = tf.transform(non_spam_words)\n",
        "\n",
        "# Label the features: 1 for spam, 0 for non-spam\n",
        "spam_samples = spam_features.map(lambda features:LabeledPoint(1, features))\n",
        "non_spam_samples = non_spam_features.map(lambda features: LabeledPoint(0, features))\n",
        "\n",
        "# Combine the two datasets\n",
        "samples = spam_samples.join(non_spam_samples)\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Split the data into training and testing\n",
        "train_samples, test_samples = samples.randomSplit([0.8,0.2])\n",
        "\n",
        "# Train the model\n",
        "model = LogisticRegressionWithLBFGS.train(train_samples)\n",
        "\n",
        "# Create a prediction label from the test data\n",
        "predictions = model.predict(test_samples.map(lambda x: x.features))\n",
        "\n",
        "# Combine original labels with the predicted labels\n",
        "labels_and_preds = test_samples.map(lambda x: x.label).zip(predictions)\n",
        "\n",
        "\n",
        "# Check the accuracy of the model on the test data\n",
        "accuracy = labels_and_preds.filter(lambda x: x[0] == x[1]).count()/float(test_samples.count())\n",
        "print(\"Model accuracy : {:.2f}\".format(accuracy))\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2TKiWtpCpZU"
      },
      "source": [
        "Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQyPZMG4Cq9t"
      },
      "source": [
        "\"\"\"\n",
        "# Load the dataset into an RDD\n",
        "clusterRDD = sc.textFile(file_path)\n",
        "\n",
        "# Split the RDD based on tab\n",
        "rdd_split = clusterRDD.map(lambda x: x.split(\"\\t\"))\n",
        "\n",
        "# Transform the split RDD by creating a list of integers\n",
        "rdd_split_int = rdd_split.map(lambda x: [int(x[0]), int(x[1])])\n",
        "\n",
        "# Count the number of rows in RDD \n",
        "print(\"There are {} rows in the rdd_split_int dataset\".format(rdd_split_int.count()))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Train the model again with the best k\n",
        "model = KMeans.train(rdd_split_int, k = , seed=1)\n",
        "\n",
        "# Get cluster centers\n",
        "cluster_centers = model.cluster_centers\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Convert rdd_split_int RDD into Spark DataFrame and then to Pandas DataFrame\n",
        "rdd_split_int_df_pandas = spark.createDataFrame(rdd_split_int, schema = ['col1','col2']).toPandas()\n",
        "\n",
        "# Convert cluster_centers to a pandas DataFrame\n",
        "cluster_centers_pandas = pd.DataFrame(cluster_centers, columns = ['col1','col2'])\n",
        "\n",
        "# Create an overlaid scatter plot of clusters and centroids\n",
        "plt.scatter(rdd_split_int_df_pandas[\"col1\"], rdd_split_int_df_pandas[\"col2\"])\n",
        "plt.scatter(cluster_centers_pandas[\"col1\"], cluster_centers_pandas[\"col2\"], color=\"red\", marker=\"x\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}