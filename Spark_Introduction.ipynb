{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_Introduction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWhR5iHssONMrxLgksTGPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverRevilla/Spark_Pyspark/blob/main/Spark_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37XW72rTG0vW"
      },
      "source": [
        "## **Spark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU1yUpk_G6Yi"
      },
      "source": [
        "Sparksession and Sparkconfig"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONDzuIyC8vLs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6acc34e9-baef-4e68-ef1d-2922cb1c8d2b"
      },
      "source": [
        "\"\"\" \n",
        "Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "# SparkContext\n",
        "# SparkSession\n",
        "\n",
        "# Print the version of spark\n",
        "from pyspark import SparkContext as sc\n",
        "sc.version\n",
        "\n",
        "# Print the version of Python\n",
        "sc.pythonVer\n",
        "\n",
        "# Print the name of the compute here it is running\n",
        "sc.master\n",
        "\n",
        "# Create my_spark\n",
        "my_spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Print my_spark\n",
        "print(my_spark) \n",
        "\n",
        "# Print the tables in the catalog\n",
        "print(spark.catalog.listTables()) \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Import SparkSession from pyspark.sql\\nfrom pyspark.sql import SparkSession\\n\\n# Create my_spark\\nmy_spark = SparkSession.builder.getOrCreate()\\n\\n# Print my_spark\\nprint(my_spark) \\n\\n# Print the tables in the catalog\\nprint(spark.catalog.listTables()) '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENbS_qdhL-Gd"
      },
      "source": [
        "QUERIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "M_l8UvYHIQWn",
        "outputId": "ae93f6de-05f6-4340-d780-61453de6d76a"
      },
      "source": [
        "'''# Don't change this query\n",
        "query = \"FROM flights SELECT * LIMIT 10\"\n",
        "\n",
        "# Get the first 10 rows of flights\n",
        "flights10 = spark.sql(query) #because the name of the session is spark.\n",
        "# Show the results\n",
        "flights10.show() '''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Don\\'t change this query\\nquery = \"FROM flights SELECT * LIMIT 10\"\\n\\n# Get the first 10 rows of flights\\nflights10 = spark.sql(query)\\n# Show the results\\nflights10.show() '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m13Cw6KSPdy8"
      },
      "source": [
        "PANDAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oYz6v-rsMBU9",
        "outputId": "6263326b-8a8f-45c2-988a-2f7e849f308b"
      },
      "source": [
        "### RDD a Pandas DataFrame\n",
        "\n",
        "\"\"\" # Query\n",
        "query = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\n",
        "\n",
        "# Run the query\n",
        "flight_counts = spark.sql(query)\n",
        "\n",
        "# Convert the results to a pandas DataFrame\n",
        "pd_counts = flight_counts.toPandas()\n",
        "\n",
        "# Print the head of pd_counts\n",
        "print(pd_counts.head())\"\"\"\n",
        "\n",
        "### Pandas DataFrame a RDD. Spark cluster\n",
        "\"\"\" # Create pd_temp\n",
        "pd_temp = pd.DataFrame(np.random.random(10))\n",
        "\n",
        "# Create spark_temp from pd_temp\n",
        "spark_temp = spark.createDataFrame(pd_temp)\n",
        "\n",
        "# Examine the tables in the catalog\n",
        "print(spark.catalog.listTables())\n",
        "\n",
        "# Add spark_temp to the catalog\n",
        "spark_temp.createOrReplaceTempView(\"temp\") # El metodo es con el nuevo spark dataframe. 'temp'\n",
        "\n",
        "# Examine the tables in the catalog again\n",
        "print(spark.catalog.listTables()) \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' # Don\\'t change this query\\nquery = \"SELECT origin, dest, COUNT(*) as N FROM flights GROUP BY origin, dest\"\\n\\n# Run the query\\nflight_counts = spark.sql(query)\\n\\n# Convert the results to a pandas DataFrame\\npd_counts = flight_counts.toPandas()\\n\\n# Print the head of pd_counts\\nprint(pd_counts.head())'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQtYevCIVDgp"
      },
      "source": [
        "LECTURA DE ARCHIVOS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "B73Qw1hXP2cY",
        "outputId": "61c7083c-baaf-41e0-a1f3-37c34c6e680c"
      },
      "source": [
        "# Filepath, spark.attributes.methods, .show()\n",
        "\"\"\" # Don't change this file path\n",
        "file_path = \"/usr/local/share/datasets/airports.csv\"\n",
        "\n",
        "# Read in the airports data\n",
        "airports = spark.read.csv(file_path, header= True)\n",
        "\n",
        "# Show the data\n",
        "airports.show() \"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' # Don\\'t change this file path\\nfile_path = \"/usr/local/share/datasets/airports.csv\"\\n\\n# Read in the airports data\\nairports = spark.read.csv(file_path, header= True)\\n\\n# Show the data\\nairports.show() '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZXPr22r1oAj"
      },
      "source": [
        "CREANDO NUEVAS COLUMNAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-risFNpVHOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "719dbfcc-5837-4d5e-d379-9c553fc3b3e3"
      },
      "source": [
        "'''# Create the DataFrame flights\n",
        "flights = spark.table(\"flights\") \n",
        "\n",
        "# Show the head\n",
        "flights.show() ### es igual que colocar head().\n",
        "\n",
        "# Add duration_hrs\n",
        "flights = flights.withColumn('duration_hrs',flights.air_time/60) ### 'name of the newcolumn', dataframe.old_column +....\n",
        "flights.show()'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Create the DataFrame flights\\nflights = spark.table(\"flights\") \\n\\n# Show the head\\nflights.show() ### es igual que colocar head().\\n\\n# Add duration_hrs\\nflights = flights.withColumn(\\'duration_hrs\\',flights.air_time/60) ### \\'name of the newcolumn\\', dataframe.old_column +....\\nflights.show()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZWcIRmOK4G6"
      },
      "source": [
        "QUERIES variantes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mtstBxI2AdN"
      },
      "source": [
        "\"\"\" Similar to .withColumn(), you can do column-wise computations within a SELECT statement. For example,\n",
        "\n",
        "SELECT origin, dest, air_time / 60 FROM flights;\n",
        "returns a table with the origin, destination, and duration in hours for each flight.\n",
        "\n",
        "Another commonly used command is WHERE. This command filters the rows of the table based on some logical condition you specify. The resulting table contains the rows where your condition is true. For example, if you had a table of students and grades you could do:\n",
        "\n",
        "SELECT * FROM students\n",
        "WHERE grade = 'A';\n",
        "to select all the columns and the rows containing information about students who got As. \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYc3PHkYMrnO"
      },
      "source": [
        "Funciones de SQL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM88JHEyMtnw"
      },
      "source": [
        "# Filter() : Filtrado de datos. Notar que se filtra el dataframe.\n",
        "\"\"\"\n",
        "Hay dos alternativas para filtrar los datos:\n",
        "flights.filter(\"air_time > 120\").show()\n",
        "flights.filter(flights.air_time > 120).show()\n",
        "\n",
        "\"\"\"\n",
        "# Select() : Selecciona las columnas del dataframe. \n",
        "# Se diferencia de .withColumn() porque select() retorna las columnas especificadas mientras que el primero todas las columnas\n",
        "\n",
        "\"\"\"\n",
        "# Select the first set of columns\n",
        "selected1 = flights.select(\"tailnum\",\"origin\",\"dest\")\n",
        "\n",
        "# Select the second set of columns\n",
        "temp = flights.select(flights.origin,flights.dest,flights.carrier)\n",
        "\n",
        "# Define first filter\n",
        "filterA = flights.origin == \"SEA\"\n",
        "\n",
        "# Define second filter\n",
        "filterB = flights.dest == \"PDX\"\n",
        "\n",
        "# Filter the data, first by filterA then by filterB\n",
        "selected2 = temp.filter(filterA).filter(filterB)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "flights.select(flights.air_time/60)\n",
        "### alias\n",
        "flights.select((flights.air_time/60).alias(\"duration_hrs\"))\n",
        "\n",
        "## expresion sql\n",
        "flights.selectExpr(\"air_time/60 as duration_hrs\")\n",
        "\"\"\"\n",
        "# Grupby()\n",
        "\n",
        "\"\"\"\n",
        "# Find the shortest flight from PDX in terms of distance\n",
        "flights.filter(flights.origin == \"PDX\").groupBy().min(\"distance\").show()\n",
        "\n",
        "# Find the longest flight from SEA in terms of air time\n",
        "flights.filter(flights.origin == \"SEA\").groupBy().max(\"air_time\").show()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "# Average duration of Delta flights\n",
        "flights.filter(flights.carrier == \"DL\").filter(flights.origin == \"SEA\").groupBy().avg(\"air_time\").show()\n",
        "\n",
        "\n",
        "# Total hours in the air\n",
        "flights.withColumn(\"duration_hrs\", flights.air_time/60).groupBy().sum(\"duration_hrs\").show()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# GroupBy and Aggregation\n",
        "\"\"\"\n",
        "# Group by tailnum\n",
        "by_plane = flights.groupBy(\"tailnum\")\n",
        "\n",
        "# Number of flights each plane made\n",
        "by_plane.count().show()\n",
        "\n",
        "# Group by origin\n",
        "by_origin = flights.groupBy(\"origin\")\n",
        "\n",
        "# Average duration of flights from PDX and SEA\n",
        "by_origin.avg(\"air_time\").show()\n",
        "\"\"\"\n",
        "\n",
        "### Funciones de agregaciòn avanzadas\n",
        "\"\"\"\n",
        "# Import pyspark.sql.functions as F\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Group by month and dest\n",
        "by_month_dest = flights.groupBy(\"month\",\"dest\")\n",
        "\n",
        "# Average departure delay by month and destination\n",
        "by_month_dest.avg(\"dep_delay\").show()\n",
        "\n",
        "# Standard deviation of departure delay\n",
        "by_month_dest.agg(F.stddev(\"dep_delay\")).show()\n",
        "\"\"\"\n",
        "\n",
        "### Joins\n",
        "# Existen muchos tipos de joins.\n",
        "# Para renombrar columnas usar .withColumnRenamed(\"oldColumn\",\"NewColumn\")\n",
        "# .join(\"second_dataframe, \"column key\", how = \"leftouter\")         ---example()\n",
        "\"\"\"\n",
        "# Examine the data\n",
        "print(airports)\n",
        "airports.show()\n",
        "\n",
        "# Rename the faa column\n",
        "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
        "\n",
        "# Join the DataFrames\n",
        "flights_with_airports = flights.join(airports,\"dest\",how = \"leftouter\")\n",
        "\n",
        "# Examine the new DataFrame\n",
        "print(flights_with_airports)\n",
        "flights_with_airports.show()\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTdsUARYf_8c"
      },
      "source": [
        "MACHINE LEARNING PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zwe8QT-jgBbp"
      },
      "source": [
        "\"\"\"\n",
        "# Rename year column\n",
        "planes = planes.withColumnRenamed(\"year\",\"plane_year\")\n",
        "\n",
        "# Join the DataFrames\n",
        "model_data = flights.join(planes,\"tailnum\", how = \"leftouter\")\n",
        "\n",
        "\"\"\"\n",
        "### Cambiar tipos de datos: Strings to Integers or Doubles\n",
        "\"\"\"\n",
        "# dataframe = dataframe.withColumn(\"col\", dataframe.col.cast(\"new_type\"))\n",
        "\n",
        "# Cast the columns to integers\n",
        "model_data = model_data.withColumn(\"arr_delay\", model_data.arr_delay.cast(\"integer\"))\n",
        "model_data = model_data.withColumn(\"air_time\", model_data.air_time.cast(\"integer\"))\n",
        "model_data = model_data.withColumn(\"month\", model_data.month.cast(\"integer\"))\n",
        "model_data = model_data.withColumn(\"plane_year\", model_data.plane_year.cast(\"integer\"))\n",
        "\n",
        "# Create the column plane_age\n",
        "model_data = model_data.withColumn(\"plane_age\", model_data.year - model_data.plane_year )\n",
        "\"\"\"\n",
        "## Para filtrar valores nulos : Ejemplo  model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL ...\")\n",
        "\"\"\"\n",
        "# Create is_late\n",
        "model_data = model_data.withColumn(\"is_late\", model_data.arr_delay > 0)\n",
        "\n",
        "# Convert to an integer\n",
        "model_data = model_data.withColumn(\"label\", model_data.is_late.cast(\"integer\"))\n",
        "\n",
        "# Remove missing values\n",
        "model_data = model_data.filter(\"arr_delay is not NULL and dep_delay is not NULL and air_time is not NULL and plane_year is not NULL\")\n",
        "\"\"\"\n",
        "### Para trabajar con pyspark.ml se necesita convertir incluso las variables categóricas en numéricas\n",
        "### pyspark.ml.features.\n",
        "### Se debe crear un StringIndexer y luego un OneHotEncoder sobre dicho index.\n",
        "\n",
        "\"\"\"\n",
        "# Create a StringIndexer\n",
        "carr_indexer = StringIndexer(inputCol = \"carrier\", outputCol = \"carrier_index\")\n",
        "\n",
        "# Create a OneHotEncoder\n",
        "carr_encoder = OneHotEncoder(inputCol = \"carrier_index\",outputCol = \"carrier_fact\")\n",
        "\n",
        "# Create a StringIndexer\n",
        "dest_indexer = StringIndexer(inputCol = \"dest\", outputCol = \"dest_index\")\n",
        "\n",
        "# Create a OneHotEncoder\n",
        "dest_encoder = OneHotEncoder(inputCol = \"dest_index\", outputCol = \"dest_fact\")\n",
        "\n",
        "\"\"\"\n",
        "# Es necesario que todas las columnas de anteriores variables categòricas estèn en una misma columna en forma de número\n",
        "# Se debe crear un  VectorAssembler()\n",
        "\n",
        "\"\"\"\n",
        "# Make a VectorAssembler\n",
        "vec_assembler = VectorAssembler(inputCols = [\"month\", \"air_time\", \"carrier_fact\", \"dest_fact\", \"plane_age\"], outputCol = \"features\")\n",
        "\n",
        "\"\"\"\n",
        "### Create a Pipeline\n",
        "# Pipeline: Es una clase dentro de pyspark.ml que combina todos los Estimators ay Transformers.\n",
        "# Pipeline (stager = Todos los elementos indexer, encoder y vector assembler)\n",
        "\n",
        "\"\"\"\n",
        "# Import Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Make the pipeline\n",
        "flights_pipe = Pipeline(stages = [dest_indexer, dest_encoder, carr_indexer, carr_encoder, vec_assembler] )\n",
        "\n",
        "\"\"\"\n",
        "### Train y Test. Ajustar y transformar los datos dentro del pipeline\n",
        "\n",
        "\"\"\"\n",
        "# Fit and transform the data\n",
        "piped_data = flights_pipe.fit(model_data).transform(model_data)\n",
        "\n",
        "\"\"\"\n",
        "# Split the data into training and test sets\n",
        "\n",
        "\"\"\"\n",
        "training, test = piped_data.randomSplit([.6, .4])\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3arOxx2oswx"
      },
      "source": [
        "UN MODELO: REGRESIÓN LOGÍSTICA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eev6jq3Qout_"
      },
      "source": [
        "# Creando el modelo\n",
        "\"\"\"# Import LogisticRegression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create a LogisticRegression Estimator\n",
        "lr = LogisticRegression()\"\"\"\n",
        "\n",
        "# Creando una CROSS VALIDATION\n",
        "\n",
        "\"\"\"\n",
        "Lo primero que se necesita para crear una validacion cruzada es \n",
        "seleccioar una forma de comparar los modelos. pyspark.ml.evaluation tiene\n",
        "diferentes clases que ayudan a evaluar diferentes modelos. Como una regresión logística\n",
        "es un modelo de clasificación binaria se usa BinaryClassificationEvaluator, esta clase \n",
        "calcula ntre otras cosas el área bajo la curva ROC (para hacer esto se debe definir metricName = \"areaUnderROC\"). \n",
        "Como se sabe a mayor área bajo la curva mejor es el modelo\n",
        "\n",
        "# Import the evaluation submodule\n",
        "import pyspark.ml.evaluation as evals\n",
        "\n",
        "# Create a BinaryClassificationEvaluator\n",
        "evaluator = evals.BinaryClassificationEvaluator(metricName = \"areaUnderROC\")\n",
        "\"\"\"\n",
        "\n",
        "# Creando un Grid\n",
        "\"\"\"\n",
        "Se necesita crear una cuadrícula de valores continuos para buscar el hiperparámetro que optimice \n",
        "el algoritmo. El submódulo pyspark.ml.tuning incluye una clase llamada ParamGridBuilder:\n",
        "Pasos:\n",
        "Importar el módulo pyspark.ml.tuning as tune\n",
        "tune.ParamGridBuilder()\n",
        ".addGrid()   # Es necesario para usar una validación cruzada\n",
        ".build()\n",
        "\n",
        "# Import the tuning submodule\n",
        "import pyspark.ml.tuning as tune\n",
        "\n",
        "# Create the parameter grid\n",
        "grid = tune.ParamGridBuilder()\n",
        "\n",
        "# Add the hyperparameter\n",
        "grid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\n",
        "grid = grid.addGrid(lr.elasticNetParam,[0,1])\n",
        "\n",
        "# Build the grid\n",
        "grid = grid.build()\n",
        "\"\"\"\n",
        "# Creando un validador\n",
        "\"\"\"\n",
        "Del submódulo pyspark.ml.tuning se obtiene el CrossValidator.\n",
        "Como pparámetros se usan:\n",
        "estimator: del modelo que se va a evaluar con la validación cruzada.\n",
        "estimatorParamMaps: todos los valores quue puede tomar el hiperparámetro\n",
        "evaluator: el evaluador creado. E este caso fue escogido el evaluador de clasificación binaria.\n",
        "\n",
        "cv = tune.CrossValidator(estimator = lr,\n",
        "                         estimatorParamMaps = grid,\n",
        "                         evaluator = evaluator\n",
        "                        )\n",
        "\"\"\"\n",
        "# Ajustando el modelo\n",
        "\n",
        "\"\"\"\n",
        "Cmo la validación cruzada en muchos números tiene un alto costo computacional\n",
        "para hacerlo en local y seleccionar el mejor modelo se debe primero ajustar el\n",
        "modelo a datos de entrenamiento y luego escoger el mejor modelo. Dicho modelo debe\n",
        "ser de clase LogisticRegressionModel.\n",
        "\n",
        "# Fit cross validation models\n",
        "models = cv.fit(training)\n",
        "\n",
        "# Extract the best model\n",
        "best_lr = models.bestModel\n",
        "\n",
        "por defecto solo corriendo para un valor se definen los siguientes parámetros\n",
        "regParam = 0\n",
        "elasticNetParam = 0\n",
        "\"\"\"\n",
        "\n",
        "# Evaluando el modelo\n",
        "\n",
        "\"\"\"\n",
        "Para realizar predcciones con el modelo propuesto s debe usar\n",
        "modelo.transform() en el conjunto de testeo \n",
        "Para evluar el modelo, se pueden usar diferentes parámetros, para este caso\n",
        "evaluator.evaluate() sobre los resultados de a prediccion\n",
        "\n",
        "Recordar que se creó un evaluador por lo que solo se agregaría el módulo\n",
        "evaluate()\n",
        "\n",
        "# Use the model to predict the test set\n",
        "test_results = best_lr.transform(test)\n",
        "\n",
        "# Evaluate the predictions\n",
        "print(evaluator.evaluate(test_results))\n",
        "\n",
        "\"\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}