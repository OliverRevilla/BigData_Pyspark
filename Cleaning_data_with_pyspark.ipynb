{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning_data_with_pyspark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe9Bn1Txtbm9+vfeSIEJsk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverRevilla/Spark_Pyspark/blob/main/Cleaning_data_with_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRFSC9GwoKDr"
      },
      "source": [
        "Introduction to data clenaning with Apache Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1KTev_OnmYR"
      },
      "source": [
        "# Schema\n",
        "\"\"\"\n",
        "import pyspark.sql.types\n",
        "peopleSchema = StructType([\n",
        "    # Define the name field\n",
        "    StructField('name', StringType(),True),\n",
        "    # Add the age field                       \n",
        "    StructField('age',IntegerType(),True),\n",
        "    # Add the city field\n",
        "    StructField('city', StringType(),True)\n",
        "])\n",
        "\n",
        "# Read CSV file containing data\n",
        "people_df = spark.read.format('csv').load(name = 'rawdata.csv', schema = peopleSchema)\n",
        "\"\"\"\n",
        "# Import the pyspark.sql.types library\n",
        "\"\"\"\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Define a new schema using the StructType method \n",
        "people_schema = StructType([\n",
        "  # Define a StructField for each fiekld\n",
        "  StructField('Name', StringType(), True),\n",
        "  StructField('Age',IntegerType(),True),\n",
        "  StructField('City',StringType(),True)\n",
        "])\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Using lazy processing\n",
        "voter_df = spark.read.csv('votedata.csv')\n",
        "voter_df = voter_df.withColumn('fullyear', voter_df.year + 2000)\n",
        "voter_df = voter_df.drop(voter_df.year)\n",
        "\n",
        "# Load the CSV file\n",
        "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
        "\n",
        "# Add the airport column using the F.lower() method\n",
        "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
        "\n",
        "# Drop the Destination Airport column\n",
        "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
        "\n",
        "# Show the DataFrame\n",
        "aa_dfw_df.show(3)\n",
        "\"\"\"\n",
        "# Understanding Parquet\n",
        "\n",
        "# Reading parquet files\n",
        "\"\"\"\n",
        "df = spark.read.format('parquet').load('filename.parquet')\n",
        "df = spark.read.parquet('filename.parquet')\n",
        "\"\"\"\n",
        "# Writing parquet files\n",
        "\"\"\"\n",
        "df.write.format('parquet').save('filename.parquet')\n",
        "df.write.parquet('filename.parquet')\n",
        "\n",
        "# View the row count of df1 and df2\n",
        "print('df1 Count: %d' % df1.count())\n",
        "print('df2 Count: %d' % df2.count())\n",
        "\n",
        "# Combine the DataFrames into one\n",
        "df3 = df1.union(df2)\n",
        "\n",
        "# Save the df3 DataFrame in Parquet format\n",
        "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
        "\n",
        "# Read the Parquet file into a new DataFrame and run a count\n",
        "print(spark.read.parquet('AA_DFW_ALL.parquet').count())\n",
        "\"\"\"\n",
        "# Parquet and SQL\n",
        "\"\"\"\n",
        "df = spark.read.parquet('df.parquet')\n",
        "df.createOrReplaceTempView('flights')\n",
        "short_flights_df = spark.sql('SELECT * FROM flights WHERE flightduration < 100')\n",
        "\n",
        "# Read the Parquet file into flights_df\n",
        "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
        "\n",
        "# Register the temp table\n",
        "flights_df.createOrReplaceTempView('flights')\n",
        "\n",
        "# Run a SQL query of the average flight duration\n",
        "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
        "print('The average flight time is: %d' % avg_duration)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeo3uggZfe2n"
      },
      "source": [
        "Manipulating DataFrames in the real world"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hp1BxM3-fpFU"
      },
      "source": [
        "# DataFrame column operations\n",
        "\n",
        "# Return rows where name starts with \"M\"\n",
        "\"\"\"\n",
        "volter_df.filter(volder_df.name.like('%M'))\n",
        "\n",
        "# Return name and position only\n",
        "voters = voter_df.select('name','position')\n",
        "\n",
        "# Common dataFrame transformations\n",
        "# Filter/where\n",
        "voter_df.filter(voter_df.date > '1/1/2019')\n",
        "voter_df.where(voter_df.date > '1/1/2019')\n",
        "# Select\n",
        "voter_df.select(voter_df.name)\n",
        "# withColumn\n",
        "voter_df.withColumn('year', voter_df.date.year)\n",
        "# drop\n",
        "voter_df.drop('unused_column')\n",
        "\n",
        "### Filtering data\n",
        "df.filter(df['name'].isNotNull())\n",
        "df.filter(df['date'] > 1000)\n",
        "df.where(df['columna'].contains('criteria'))\n",
        "df.where(df.columna.isNull())\n",
        "\"\"\"\n",
        "### Column string transformations\n",
        "\"\"\"\n",
        "import pyspark.sql.functions as F\n",
        "# Applied per column as transformation\n",
        "df.withColumn('upper', F.upper('name')) # withColumn('NewColumns', F.function('OldColumn'))\n",
        "# Can create intermediary columns\n",
        "voter_df.withColumn('splits', F.split('name', ''))\n",
        "# Can cast to other types\n",
        "df.withColumn('year',df['c4'].cast(IntegerType()))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# ArrayType() column functions\n",
        ".size() # Returns length of arrayType() column\n",
        ".getItem() # Used to retrieve a specific item at index of list column\n",
        "\n",
        "import pyspark.sql.funtions as F\n",
        "\n",
        "# Show the distinct VOTER_NAME entries\n",
        "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
        "\n",
        "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
        "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
        "\n",
        "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
        "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
        "\n",
        "# Show the distinct VOTER_NAME entries again\n",
        "voter_df.select('VOTER_NAME').distinct().show(40, truncate = False)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "#Exercise\n",
        "# Add a new column called splits separated on whitespace\n",
        "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
        "\n",
        "# Create a new column called first_name based on the first item in splits\n",
        "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
        "\n",
        "# Get the last entry of the splits list and create a column called last_name\n",
        "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))\n",
        "\n",
        "# Drop the splits column\n",
        "voter_df = voter_df.drop('splits')\n",
        "\n",
        "# Show the voter_df DataFrame\n",
        "voter_df.show()\n",
        "\"\"\"\n",
        "# Conditional DataFrame column operations\n",
        "# when(\"condicion\", then...)\n",
        "\"\"\"\n",
        "\"\"Individual when()\n",
        "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\"))\n",
        "\"\" Multiple when()\n",
        "df.select(df.Name, df.Age,\n",
        "          .when(df.Age >= 18, \"Adult)\n",
        "          .when(df.Age < 18, \"Minor\"))\n",
        "\n",
        "df.select(df.Name, df.Age,\n",
        "          .when(df.Age >= 18, \"Adult)\n",
        "          .otherwise(\"Minor\"))\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Add a column to voter_df for any voter with the title **Councilmember**\n",
        "voter_df = voter_df.withColumn('random_val',\n",
        "                               when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
        "\n",
        "# Show some of the DataFrame rows, noting whether the when clause worked\n",
        "voter_df.show()\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Add a column to voter_df for a voter based on their position\n",
        "voter_df = voter_df.withColumn('random_val',\n",
        "                               when(voter_df.TITLE ==  'Councilmember', F.rand())\n",
        "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
        "                               .otherwise(0))\n",
        "\n",
        "# Show some of the DataFrame rows\n",
        "voter_df.show()\n",
        "\n",
        "# Use the .filter() clause with random_val\n",
        "voter_df.filter(voter_df.random_val == 0).show()\n",
        "\n",
        "\"\"\"\n",
        "# User defined functions\n",
        "#pyspark.sql.functions.udf()\n",
        "\n",
        "\"\"\"\n",
        "def reverseString(mystr):\n",
        "  return mystr(::-1)\n",
        "\n",
        "udfReverseString = F.udf(reverseString, StringType()) # Sintaxis (function that we defined, data type)\n",
        "\n",
        "# Use with Spark\n",
        "user_df = user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def sortingCap():\n",
        "  return random.choice(['G','H','R','S'])\n",
        "\n",
        "udfSortingCap = udf(sortingCap, StringType())\n",
        "user_df = user_df.withColumn('Class', udfSortingCap())\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def getFirstAndMiddle(names):\n",
        "  # Return a space separated string of names\n",
        "  return ' '.join(names)\n",
        "\n",
        "# Define the method as a UDF\n",
        "udfFirstAndMiddle = F.udf(getFirstAndMiddle,StringType())\n",
        "\n",
        "# Create a new column using your UDF\n",
        "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
        "\n",
        "# Show the DataFrame\n",
        "voter_df.show()\n",
        "\"\"\"\n",
        "## IDs\n",
        "\"\"\"\n",
        "# Select all the unique council voters\n",
        "voter_df = df.select(df['VOTER NAME']).distinct()\n",
        "\n",
        "# Count the rows in voter_df\n",
        "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
        "\n",
        "# Add a ROW_ID\n",
        "voter_df = voter_df.withColumn('ROW_ID',F.monotonically_increasing_id())\n",
        "\n",
        "# Show the rows with 10 highest IDs in the set\n",
        "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Print the number of partitions in each DataFrame\n",
        "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
        "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
        "\n",
        "# Add a ROW_ID field to each DataFrame\n",
        "voter_df = voter_df.withColumn('ROW_ID',F.monotonically_increasing_id() )\n",
        "\n",
        "voter_df_single = voter_df_single.withColumn('ROW_ID',F.monotonically_increasing_id() )\n",
        "\n",
        "# Show the top 10 IDs in each DataFrame \n",
        "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
        "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)\n",
        "\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Determine the highest ROW_ID and save it in previous_max_ID\n",
        "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
        "\n",
        "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
        "voter_df_april = voter_df_april.withColumn('ROW_ID', previous_max_ID + F.monotonically_increasing_id())\n",
        "\n",
        "# Show the ROW_ID from both DataFrames and compare\n",
        "voter_df_march.select('ROW_ID').show()\n",
        "voter_df_april.select('ROW_ID').show()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV73d8JpN3jJ"
      },
      "source": [
        "Caching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "RQ2jnDzNPT45",
        "outputId": "8c27b8e7-e378-4525-b2c9-d96a8e0d51f8"
      },
      "source": [
        "# Implementiing caching\n",
        "\"\"\"df = spark.read_csv('voter_data.txt.gz')\n",
        "df.cache().count()\n",
        "\n",
        "df = df.withColumn('ID', monotically_incrasing_id())\n",
        "df = df.cache()\n",
        "df.show()\n",
        "\n",
        "#  check if a dataframe is cached\n",
        "print(voter_df.is_cached)\n",
        "\n",
        "# finish cached\n",
        ".unpersist()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\"\"\"\n",
        "# Add caching to the unique rows in departures_df\n",
        "\"\"\"\n",
        "departures_df = departures_df.distinct().cache()\n",
        "\n",
        "# Count the unique rows in departures_df, noting how long the operation takes\n",
        "print(\"Counting %d rows took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
        "\n",
        "# Count the rows again, noting the variance in time of a cached DataFrame\n",
        "start_time = time.time()\n",
        "print(\"Counting %d rows again took %f seconds\" % (departures_df.count(), time.time() - start_time))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Determine if departures_df is in the cache\n",
        "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
        "print(\"Removing departures_df from cache\")\n",
        "\n",
        "# Remove departures_df from the cache\n",
        "departures_df.unpersist()\n",
        "\n",
        "# Check the cache status again\n",
        "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
        "\"\"\"\n",
        "# Optimize\n",
        "\"\"\"\n",
        "df_csv = spark.read.csv('df.csv')\n",
        "df_csv.write.parquet('data.parquet')\n",
        "df = spark.read.parquet('data.parquet')\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Import the full and split files into DataFrames\n",
        "full_df = spark.read.csv('departures_full.txt.gz')\n",
        "split_df = spark.read.csv('departures_000.txt.gz')\n",
        "\n",
        "# Print the count and run time for each DataFrame\n",
        "start_time_a = time.time()\n",
        "print(\"Total rows in full DataFrame:\\t%d\" % full_df.count())\n",
        "print(\"Time to run: %f\" % (time.time() - start_time_a))\n",
        "\n",
        "start_time_b = time.time()\n",
        "print(\"Total rows in split DataFrame:\\t%d\" % split_df.count())\n",
        "print(\"Time to run: %f\" % (time.time() - start_time_b))\n",
        "\"\"\"\n",
        "# Cluster sizing types\n",
        "\"\"\"\n",
        "spark.conf.get()\n",
        "spark.conf.set().\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Name of the Spark application instance\n",
        "app_name = spark.conf.get('spark.app.name')\n",
        "\n",
        "# Driver TCP port\n",
        "driver_tcp_port = spark.conf.get('spark.driver.port')\n",
        "\n",
        "# Number of join partitions\n",
        "num_partitions = spark.conf.get('spark.sql.shuffle.partitions')\n",
        "\n",
        "# Show the results\n",
        "print(\"Name: %s\" % app_name)\n",
        "print(\"Driver TCP port: %s\" % driver_tcp_port)\n",
        "print(\"Number of partitions: %s\" % num_partitions)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Store the number of partitions in variable\n",
        "before = departures_df.rdd.getNumPartitions()\n",
        "\n",
        "# Configure Spark to use 500 partitions\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
        "\n",
        "# Recreate the DataFrame using the departures data file\n",
        "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
        "\n",
        "# Print the number of partitions for each instance\n",
        "print(\"Partition count before change: %d\" % before)\n",
        "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())\n",
        "\"\"\"\n",
        "# Performance improvements\n",
        "\"\"\"\n",
        "voter_df = df.select(df['VOTER NAME']).distinct()\n",
        "voter_df.explain()\n",
        "# Shuffling refers to moving data around to varios workers to complete a task\n",
        "# How to limit shuffling?\n",
        ".repartition(num_partitions)\n",
        ".coalesce(num_partitions)\n",
        ".join()\n",
        ".broadcast(): Provides a copy of an object to each worker\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "combined_df = df_1.join(broadcast(df_2)) --- is adviceable for small dataframes or F.broadcast\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Join the flights_df and airports_df DataFrames\n",
        "normal_df = flights_df.join(airports_df,\\\n",
        "            flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
        "\n",
        "# Show the query plan\n",
        "normal_df.explain()\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Import the broadcast method from pyspark.sql.functions\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Join the flights_df and airports_df DataFrames using broadcasting\n",
        "broadcast_df = flights_df.join(broadcast(airports_df), \\\n",
        "    flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\n",
        "\n",
        "# Show the query plan and compare against the original\n",
        "broadcast_df.explain()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Import the broadcast method from pyspark.sql.functions\\nfrom pyspark.sql.functions import broadcast\\n\\n# Join the flights_df and airports_df DataFrames using broadcasting\\nbroadcast_df = flights_df.join(broadcast(airports_df),     flights_df[\"Destination Airport\"] == airports_df[\"IATA\"] )\\n\\n# Show the query plan and compare against the original\\nbroadcast_df.explain()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcikINE8fDxs"
      },
      "source": [
        "Quick pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8scCuJ6OezHj"
      },
      "source": [
        "\"\"\"\n",
        "# Import the data to a DataFrame\n",
        "departures_df = spark.read.csv('2015-departures.csv.gz', header=True)\n",
        "\n",
        "# Remove any duration of 0\n",
        "departures_df = departures_df.filter(departures_df[3] > 0)\n",
        "\n",
        "# Add an ID column\n",
        "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
        "\n",
        "# Write the file out to JSON format\n",
        "departures_df.write.json('output.json', mode='overwrite')\n",
        "\"\"\"\n",
        "#change the type of data\n",
        "\"\"\"\n",
        "departures_df = departures_df.withColumn('Duration', departures_df['Duration'].cast(IntegerType()))\n",
        "\"\"\"\n",
        "\n",
        "# Removing blank lines, headers, and comments\n",
        "# Removing comments\n",
        "\"\"\"\n",
        "df1 = spark.read.csv('datafile.csv.gz', comment = '#')\n",
        "\"\"\"\n",
        "# Automatic column creation\n",
        "\"\"\"\n",
        "df1 = spark.read.csv('df.csv.gz', sep = ',')\n",
        "df1 = spark.read.csv('df.csv.gz', sep = '*')\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Import the file to a DataFrame and perform a row count\n",
        "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
        "full_count = annotations_df.count()\n",
        "\n",
        "# Count the number of rows beginning with '#'\n",
        "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
        "\n",
        "# Import the file to a new DataFrame, without commented rows\n",
        "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
        "\n",
        "# Count the new DataFrame and verify the difference is as expected\n",
        "no_comments_count = no_comments_df.count()\n",
        "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Split _c0 on the tab character and store the list in a variable\n",
        "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
        "\n",
        "# Create the colcount column on the DataFrame\n",
        "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
        "\n",
        "# Remove any rows containing fewer than 5 fields\n",
        "annotations_df_filtered = annotations_df.filter(~ (F.col('colcount') < 5))\n",
        "\n",
        "# Count the number of rows\n",
        "final_count = annotations_df_filtered.count()\n",
        "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Split the content of _c0 on the tab character (aka, '\\t)\n",
        "split_cols = F.split(annotations_df['_c0'],'\\t')\n",
        "\n",
        "# Add the columns folder, filename, width, and height\n",
        "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
        "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
        "split_df = split_df.withColumn('width',split_cols.getItem(2))\n",
        "split_df = split_df.withColumn('height',split_cols.getItem(3))\n",
        "\n",
        "# Add split_cols as a column\n",
        "split_df = split_df.withColumn('split_cols', split_cols)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "def retriever(cols,colcount):\n",
        "  # Return a list of dog data\n",
        "  return cols[4:colcount]\n",
        "\n",
        "# Define the method as a UDF\n",
        "udfRetriever = F.udf(retriever,ArrayType(StringType()))\n",
        "\n",
        "# Create a new column using your UDF\n",
        "split_df = split_df.withColumn('dog_list', udfRetriever(split_df['split_cols'],split_df['colcount']))\n",
        "\n",
        "# Remove the original column, split_cols, and the colcount\n",
        "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')\n",
        "\"\"\"\n",
        "# Validation\n",
        "\n",
        "#Validating via joins\n",
        "parsed_df = spark.read.parquet('parsed_data.parquet')\n",
        "company_df = spark.read.parquet('companies.parquet')\n",
        "verfied_df = parsed_df.join(company_df, parsed_df.company == company_df.company) # thi is an inner join\n",
        "\n",
        "\"\"\"\n",
        "# Rename the column in valid_folders_df\n",
        "valid_folders_df = valid_folders_df.withColumnRenamed('_c0','folder')\n",
        "\n",
        "# Count the number of rows in split_df\n",
        "split_count = split_df.count()\n",
        "\n",
        "# Join the DataFrames\n",
        "joined_df = split_df.join(valid_folders_df, \"folder\")\n",
        "\n",
        "# Compare the number of rows remaining\n",
        "joined_count = joined_df.count()\n",
        "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Determine the row counts for each DataFrame\n",
        "split_count = split_df.count()\n",
        "joined_count = joined_df.count()\n",
        "\n",
        "# Create a DataFrame containing the invalid rows\n",
        "invalid_df = split_df.join(F.broadcast(joined_df), 'folder')\n",
        "\n",
        "# Validate the count of the new DataFrame is as expected\n",
        "invalid_count = invalid_df.count()\n",
        "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
        "\n",
        "# Determine the number of distinct folder rows removed\n",
        "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
        "print(\"%d distinct invalid folders found\" % invalid_folder_count)\n",
        "\"\"\"\n",
        "# Final Analysis and delivery\n",
        "\"\"\"\n",
        "def getAvgSale(saleslist):\n",
        "  totalsales = 0\n",
        "  count = 0\n",
        "  for sale in saleslist:\n",
        "    totalsales += sale[2] + sale[3] \n",
        "    count += 2\n",
        "  return totalsales/count\n",
        "udfGetAvgSale = udf(getAvgSale, DoubleType())\n",
        "df =  df.withColumn('avg_sale',udfGetAvgSale(df.sales_list))\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Select the dog details and show 10 untruncated rows\n",
        "print(joined_df.select('dog_list').show(10, truncate=False))\n",
        "\n",
        "# Define a schema type for the details in the dog list\n",
        "DogType = StructType([\n",
        "    StructField('breed',StringType(),False),\n",
        "    StructField('start_x',IntegerType(),False),\n",
        "    StructField('start_y',IntegerType(),False),\n",
        "    StructField('end_x',IntegerType(),False),\n",
        "    StructField('end_y',IntegerType(),False)\n",
        "])\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Create a function to return the number and type of dogs as a tuple\n",
        "def dogParse(doglist):\n",
        "  dogs = []\n",
        "  for dog in doglist:\n",
        "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
        "    dogs.append((breed, int(start_x), int(start_y), int(end_x),int(end_y)))\n",
        "  return dogs\n",
        "\n",
        "# Create a UDF\n",
        "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
        "\n",
        "# Use the UDF to list of dogs and drop the old column\n",
        "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
        "\n",
        "# Show the number of dogs in the first 10 rows\n",
        "joined_df.select(F.size('dogs')).show(10)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Define a UDF to determine the number of pixels per image\n",
        "def dogPixelCount(doglist):\n",
        "  totalpixels = 0\n",
        "  for dog in doglist:\n",
        "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
        "  return totalpixels\n",
        "\n",
        "# Define a UDF for the pixel count\n",
        "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
        "joined_df = joined_df.withColumn('dog_pixels', udfDogPixelCount('dogs'))\n",
        "\n",
        "# Create a column representing the percentage of pixels\n",
        "joined_df = joined_df.withColumn('dog_percent', (joined_df.dog_pixels / (joined_df.width * joined_df.height)) * 100)\n",
        "\n",
        "# Show the first 10 annotations with more than 60% dog\n",
        "joined_df.where('dog_percent > 60').show(10)\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}