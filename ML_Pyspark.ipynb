{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Pyspark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNuLHih1sxyKz+xIEmX6cW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OliverRevilla/BigData_Pyspark/blob/main/ML_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EczzfrNVCLnh"
      },
      "source": [
        "## **Classification**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1r8v99mN5dAV"
      },
      "source": [
        "##**Data Preparation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s883WgPY5s5B"
      },
      "source": [
        "**Drop Columns**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgO8sZWU5L0v"
      },
      "source": [
        "# Either drop the columns you don't want\n",
        "cars = df.drop('maker','model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuATg6bq6csV"
      },
      "source": [
        "**Filtering out missing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKh4gxCc6hX5"
      },
      "source": [
        "# How many missing values?\n",
        "cars.filter('cyl IS NULL').count()\n",
        "\n",
        "# Drop records with missing values in the cylinders column\n",
        "cars = cars.filter('cyl IS NOT NULL')\n",
        "\n",
        "# Drop records with missing values in any column \n",
        "cars = cars.dropna(how = 'All', subset = 'col1') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u590_wRx8HJv"
      },
      "source": [
        "**Mutating columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqWvNsa58Lai"
      },
      "source": [
        "from pyspark.sql.functions import round\n",
        "cars = cars.withColumn('mass',round(cars.weight/2.205,0))\n",
        "#-------------------------------------------------------\n",
        "from pyspark.sql.functions import round\n",
        "\n",
        "# Convert 'mile' to 'km' and drop 'mile' column\n",
        "flights_km = flights.withColumn('km',round(flights['mile']*1.60934,0))\\\n",
        "                    .drop('mile')\n",
        "\n",
        "# Create 'label' column indicating whether flight delayed (1) or not (0)\n",
        "flights_km = flights_km.withColumn('label', (flights_km['delay'] >= 15).cast('integer'))\n",
        "\n",
        "# Check first five records\n",
        "flights_km.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWKHvYXo9TNR"
      },
      "source": [
        "**Indexing categorical data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VvCeAYt9YZ-"
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexer = StringIndexer(inputCol = 'type',\n",
        "                        outputCol = 'type_idx')\n",
        "\n",
        "# Classification\n",
        "# Assign index values to strings\n",
        "# During the setting process it identify all values of the column and assing one indexer to each value\n",
        "indexer = indexer.fit(cars)\n",
        "\n",
        "# Then the model creates a new column with all values of indexes\n",
        "cars = indexer.transform(cars)\n",
        "\n",
        "# If is neccesary sort the indexes use stringOrderType\n",
        "# Import the required function\n",
        "# -----------------------------------------------------------------------------------\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "# Create an indexer\n",
        "indexer = StringIndexer(inputCol= 'carrier', outputCol='carrier_idx')\n",
        "\n",
        "# Indexer identifies categories in the data\n",
        "indexer_model = indexer.fit(flights)\n",
        "\n",
        "# Indexer creates a new column with numeric index values\n",
        "flights_indexed = indexer_model.transform(flights)\n",
        "\n",
        "# Repeat the process for the other categorical feature\n",
        "flights_indexed = StringIndexer(inputCol='org', outputCol='org_idx').fit(flights_indexed).transform(flights_indexed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofG0ntvz_ECF"
      },
      "source": [
        "**Assembling columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeCadL1h_Nym"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "assembler = VectorAssembler(inputCols = ['cyl','size'], outputCol = 'features')\n",
        "# In features columns join all predictors of the model\n",
        "assembler.transform(cars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CoL6ZBIGXcN"
      },
      "source": [
        "# ---------------------------------------------------------------------------------------\n",
        "# Remove the 'flight' column\n",
        "flights_drop_column = flights.drop('flight')\n",
        "\n",
        "# Number of records with missing 'delay' values\n",
        "flights_drop_column.filter('delay IS NULL').count()\n",
        "\n",
        "# Remove records with missing 'delay' values\n",
        "flights_valid_delay = flights_drop_column.filter('delay IS NOT NULL')\n",
        "\n",
        "# Remove records with missing values in any column and get the number of remaining rows\n",
        "flights_none_missing = flights_valid_delay.dropna()\n",
        "print(flights_none_missing.count())\n",
        "# ----------------------------------------------------------------------------------------\n",
        "# Import the necessary class\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Create an assembler object\n",
        "assembler = VectorAssembler(inputCols=[\n",
        "    'mon','dom','dow','carrier_idx','org_idx','km','depart','duration'\n",
        "], outputCol='features')\n",
        "\n",
        "# Consolidate predictor columns\n",
        "flights_assembled = assembler.transform(flights)\n",
        "\n",
        "# Check the resulting column\n",
        "flights_assembled.select('features', 'delay').show(5, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Io-oL2WB9hy"
      },
      "source": [
        "## **Decision Tree Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDI8kAbkCdIR"
      },
      "source": [
        "**Train/Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJulLZjDB8ki"
      },
      "source": [
        "# Specify a seed for reproducibility\n",
        "cars_train, cars_test = cars.randomSplit([0.8,0.2], seed = 23)\n",
        "#------------------------------------------------------------------------\n",
        "# Split into training and testing sets in a 80:20 ratio\n",
        "flights_train, flights_test = flights.randomSplit([0.8,0.2], seed = 17)\n",
        "\n",
        "# Check that training set has around 80% of records\n",
        "training_ratio = flights_train.count() / flights.count()\n",
        "print(training_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7e88KXODA9h"
      },
      "source": [
        " **Buil a Decision Tree model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evGwTQIpDFxs"
      },
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "model_algorithm = DecisionTreeClassifier()\n",
        "tree_model = model_algorithm.fit(cars_train)\n",
        "prediction = tree_model.transform(cars_test)\n",
        "#------------------------------------------------------------\n",
        "# Import the Decision Tree Classifier class\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "# Create a classifier object and fit to the training data\n",
        "tree = DecisionTreeClassifier()\n",
        "tree_model = tree.fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "prediction = tree_model.transform(flights_test)\n",
        "prediction.select('label', 'prediction', 'probability').show(5, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JttCZkz7DpV6"
      },
      "source": [
        "**Confusion matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy1ie9J6Drra"
      },
      "source": [
        "# Is a table which describes performance of a model on testing data\n",
        "prediction.groupBy('label','prediction').count().show()\n",
        "#-------------------------------------------------------------------\n",
        "# Create a confusion matrix\n",
        "prediction.groupBy('label', 'prediction').count().show()\n",
        "\n",
        "# Calculate the elements of the confusion matrix\n",
        "TN = prediction.filter('prediction = 0 AND label = prediction').count()\n",
        "TP = prediction.filter('prediction = 1 AND label = prediction').count()\n",
        "FN = prediction.filter('prediction = 0 AND label <> prediction').count()\n",
        "FP = prediction.filter('prediction = 1 AND label <> prediction').count()\n",
        "\n",
        "# Accuracy measures the proportion of correct predictions\n",
        "accuracy = (TN + TP)/(TN + TP + FN + FP)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPwrJ_cTNPHA"
      },
      "source": [
        "## **Logistic Regression Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riUdr3muNRL5"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create a logistic Regression Classifier\n",
        "logistic = LogisticRegression()\n",
        "\n",
        "# Learn from the training data\n",
        "logistic = logistic.fit(cars_train)\n",
        "\n",
        "# Predictions\n",
        "prediction = logistic.transform(cars_test)\n",
        "#--------------------------------------------------------------\n",
        "# Import the logistic regression class\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Create a classifier object and train on training data\n",
        "logistic = LogisticRegression().fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and show confusion matrix\n",
        "prediction = logistic.transform(flights_test)\n",
        "prediction.groupBy('label', 'prediction').count().show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_by6S_eTEZo"
      },
      "source": [
        "# Precision and recall\n",
        "# Precision\n",
        "TP/(TP + FP)\n",
        "# Recall\n",
        "TP/(TP + FN)\n",
        "\n",
        "# Weighted metrics\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "evaluator.evaluate(prediction,{evaluator,metricName: 'weightedPrecision'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGktrDibVCE4"
      },
      "source": [
        "## **Turning Text Into tables**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzUu4xolVHNr"
      },
      "source": [
        "# Removing punctuation\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# Regular expression (REGEX) to match commas and hyphens\n",
        "REGEX = '[,\\\\-]'\n",
        "\n",
        "books = books.withColumn('text',regexp_replace(books.text,REGEX,''))\n",
        "\n",
        "# Text to tokens\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "books = Tokenizer(inputCol = 'text',outputCol = \"tokens\").transform(books)\n",
        "\n",
        "# Stop words\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "stopwords = StopWordsRemover()\n",
        "stopwords.getStopWords()\n",
        "\n",
        "# Removing stop words\n",
        "# Specify the input and output column names\n",
        "stopwords = stopwords.setInputCol('tokens').setOutputCol('words')\n",
        "books = stopwords.transform(books)\n",
        "\n",
        "# Feature hashing\n",
        "from pyspark.ml.feature import HashingTF\n",
        "hasher = HashingTF(inputCol = 'words',outputCol = 'hash',numFeatures = 32)\n",
        "books = hasher.transform(books)\n",
        "\n",
        "# Dealing with common words\n",
        "from pyspark.ml.feature import IDF\n",
        "books = IDF(inputCol = 'hash', outputCol = 'features').fit(books).transform(books)\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# Import the necessary functions\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "\n",
        "# Remove punctuation (REGEX provided) and numbers\n",
        "wrangled = sms.withColumn('text', regexp_replace(sms.text, '[_():;,.!?\\\\-]', ' '))\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text,'[0-9]', ' '))\n",
        "\n",
        "# Merge multiple spaces\n",
        "wrangled = wrangled.withColumn('text', regexp_replace(wrangled.text, ' +', ' '))\n",
        "\n",
        "# Split the text into words\n",
        "wrangled = Tokenizer(inputCol='text', outputCol='words').transform(wrangled)\n",
        "\n",
        "wrangled.show(4, truncate=False)\n",
        "\n",
        "from pyspark.ml.feature import StopWordsRemover,HashingTF,IDF\n",
        "\n",
        "# Remove stop words.\n",
        "wrangled = StopWordsRemover(inputCol='words', outputCol='terms')\\\n",
        "      .transform(sms)\n",
        "\n",
        "# Apply the hashing trick\n",
        "wrangled = HashingTF(inputCol = 'terms', outputCol = 'hash', numFeatures=1024)\\\n",
        "      .transform(wrangled)\n",
        "\n",
        "# Convert hashed symbols to TF-IDF\n",
        "tf_idf = IDF(inputCol = 'hash', outputCol = 'features')\\\n",
        "      .fit(wrangled).transform(wrangled)\n",
        "      \n",
        "tf_idf.select('terms', 'features').show(4, truncate=False)\n",
        "# Model\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "sms_train, sms_test = sms.randomSplit([0.8,0.2], seed = 13)\n",
        "\n",
        "# Fit a Logistic Regression model to the training data\n",
        "logistic = LogisticRegression(regParam=0.2).fit(sms_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "prediction = logistic.transform(sms_test)\n",
        "\n",
        "# Create a confusion matrix, comparing predictions to known labels\n",
        "prediction.groupBy('label','prediction').count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVIZurm-Le5R"
      },
      "source": [
        "## **Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sin3d-KTMKw3"
      },
      "source": [
        "## **One-Hot Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N9IELxgLhSK"
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "onehot = OneHotEncoderEstimator(inputCols = ['type_idx'], outputCols = ['type_dummy'])\n",
        "\n",
        "# Fit the encoder to the data\n",
        "onehot = onehot.fit(cars)\n",
        "\n",
        "# How many category levels?\n",
        "onehot.categorySizes\n",
        "\n",
        "cars = onehot.transform(cars)\n",
        "cars.select('type','typer_idx','type_dummy').distinct().sort('type_idx').show()\n",
        "\n",
        "# Dense versus sparse\n",
        "from pyspark.mllib.linalg import DenseVector, SparseVector\n",
        "\n",
        "# --------------------------------------------------------------------------------\n",
        "# Import the one hot encoder class\n",
        "from pyspark.ml.feature import OneHotEncoderEstimator\n",
        "\n",
        "# Create an instance of the one hot encoder\n",
        "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols =  ['org_dummy'])\n",
        "\n",
        "# Apply the one hot encoder to the flights data\n",
        "onehot = onehot.fit(flights)\n",
        "flights_onehot = onehot.transform(flights)\n",
        "\n",
        "# Check the results\n",
        "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K28E-xB5SMKZ"
      },
      "source": [
        "## **Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZSUQyyJSNr8"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "regression = LinearRegression(labelCol = 'consuption')\n",
        "\n",
        "regression = regression.fit(cars_train)\n",
        "predictions = regression.transform(cars_test)\n",
        "\n",
        "# Examine intercept\n",
        "regression.intercept\n",
        "\n",
        "# Examine Coefficients\n",
        "regression.coefficients\n",
        "\n",
        "\n",
        "# Calculate RMSE\n",
        "from pyspark.ml.evaluation import RegressionEvalkuator\n",
        "\n",
        "RegressionEvaluator(labelCol = 'consumption').evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "#---------------------------------------------------------------------------------------------------\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Create a regression object and train on training data\n",
        "regression = LinearRegression(labelCol = 'duration').fit(flights_train)\n",
        "\n",
        "# Create predictions for the testing data and take a look at the predictions\n",
        "predictions = regression.transform(flights_test)\n",
        "predictions.select('duration', 'prediction').show(5, False)\n",
        "\n",
        "# Calculate the RMSE\n",
        "RegressionEvaluator(labelCol = 'duration').evaluate(predictions, {RegressionEvaluator(labelCol = 'duration').metricName: 'mae'})\n",
        "\n",
        "# Intercept (average minutes on ground)\n",
        "inter = regression.intercept\n",
        "print(inter)\n",
        "\n",
        "# Coefficients\n",
        "coefs = regression.coefficients\n",
        "print(coefs)\n",
        "\n",
        "# Average minutes per km\n",
        "minutes_per_km = regression.coefficients[0]\n",
        "print(minutes_per_km)\n",
        "\n",
        "# Average speed in km per hour\n",
        "avg_speed =  60/ minutes_per_km\n",
        "\n",
        "print(avg_speed)\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5zsSSr9g1pd"
      },
      "source": [
        "## **Bucketing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vftq9-ojg6EC"
      },
      "source": [
        "from pyspark.ml.feature import Bucketizer\n",
        "\n",
        "bucketizer = Bucketizer(splits = [3500,4500,6000,6500],\n",
        "                        inputCol = 'rpm',\n",
        "                        outpulCol = 'rpm_bin')\n",
        "# Applying buckets\n",
        "cars = bucketizer.transform(cars)\n",
        "\n",
        "#------------------------------------------------------------------------\n",
        "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
        "\n",
        "# Create buckets at 3 hour intervals through the day\n",
        "buckets = Bucketizer(splits=[0,3,6,9,12,15,18,21,24], inputCol = 'depart', outputCol = 'depart_bucket')\n",
        "\n",
        "# Bucket the departure times\n",
        "bucketed = buckets.transform(flights)\n",
        "bucketed.select('depart','depart_bucket').show(5)\n",
        "\n",
        "# Create a one-hot encoder\n",
        "onehot = OneHotEncoderEstimator(inputCols = ['depart_bucket'], outputCols = ['depart_dummy'])\n",
        "\n",
        "# One-hot encode the bucketed departure times\n",
        "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
        "flights_onehot.select('depart','depart_bucket','depart_dummy').show(5)\n",
        "\n",
        "# Find the RMSE on testing data\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "RegressionEvaluator(labelCol = 'duration').evaluate(predictions)\n",
        "\n",
        "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
        "avg_eve_ogg = regression.intercept\n",
        "print(avg_eve_ogg)\n",
        "\n",
        "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
        "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
        "print(avg_night_ogg)\n",
        "\n",
        "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
        "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8]\n",
        "\n",
        "print(avg_night_jfk)\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Fit Lasso model (α = 1) to training data\n",
        "regression = LinearRegression(labelCol = 'duration', regParam = 1, elasticNetParam=1).fit(flights_train)\n",
        "\n",
        "predictions = regression.transform(flights_test)\n",
        "\n",
        "# Calculate the RMSE on testing data\n",
        "rmse = RegressionEvaluator(labelCol = 'duration').evaluate(regression.transform(flights_test))\n",
        "print(\"The test RMSE is\", rmse)\n",
        "\n",
        "# Look at the model coefficients\n",
        "coeffs = regression.coefficients\n",
        "print(coeffs)   \n",
        "\n",
        "# Number of zero coefficients\n",
        "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
        "print(\"Number of coefficients equal to 0:\", zero_coeff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz5K4ACrpxd9"
      },
      "source": [
        "## **Ensembles & Pipelines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps6xqZX5qIqC"
      },
      "source": [
        "## **Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhlBo4Trp0s2"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Create a Pipeline to specify all stages of the process\n",
        "pipeline = Pipeline(stages = [indexes,onehot,assemble,regression])\n",
        "\n",
        "# Only is necessary call to fit() method to canalize the train set \n",
        "pipeline = pipeline.fit(cars_train)\n",
        "\n",
        "# Transform() method to make predictions\n",
        "pipeline = pipeline.transform(cars_test)\n",
        "\n",
        "# Access to each stage of pipeline\n",
        "pipeline.stage[3] \n",
        "pipeline.stage[3].intercept\n",
        "pipeline.stage[3]-coefficients\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Convert categorical strings to index values\n",
        "indexer = StringIndexer(inputCol = 'org', outputCol = 'org_idx')\n",
        "\n",
        "# One-hot encode index values\n",
        "onehot = OneHotEncoderEstimator(\n",
        "    inputCols= ['org_idx','dow'],\n",
        "    outputCols= ['org_dummy','dow_dummy']\n",
        ")\n",
        "\n",
        "# Assemble predictors into a single column\n",
        "assembler = VectorAssembler(inputCols=['km','org_dummy','dow_dummy'], outputCol= 'features')\n",
        "\n",
        "# A linear regression object\n",
        "regression = LinearRegression(labelCol='duration')\n",
        "\n",
        "# Import class for creating a pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Construct a pipeline\n",
        "pipeline = Pipeline(stages=[indexer,onehot,assembler,regression])\n",
        "\n",
        "# Train the pipeline on the training data\n",
        "pipeline = pipeline.fit(flights_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "predictions = pipeline.transform(flights_test)\n",
        "\n",
        "#### Pipeline of logistic regression\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "\n",
        "# Break text into tokens at non-word characters\n",
        "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol= tokenizer.getOutputCol(), outputCol='terms')\n",
        "\n",
        "# Apply the hashing trick and transform to TF-IDF\n",
        "hasher = HashingTF(inputCol= remover.getOutputCol(), outputCol=\"hash\")\n",
        "idf = IDF(inputCol= hasher.getOutputCol(), outputCol=\"features\")\n",
        "\n",
        "# Create a logistic regression object and add everything to a pipeline\n",
        "logistic = LogisticRegression()\n",
        "pipeline = Pipeline(stages=[tokenizer, remover, hasher, idf, logistic])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs8AitB4jjdx"
      },
      "source": [
        "## **Cross-Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx0dFwTpjnEE"
      },
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# A grid of parameter values (empty for the moment)\n",
        "params = ParamGridBuilder().build()\n",
        "\n",
        "# Cross-Validation object\n",
        "cv = CrossValidator(estimator = regression,\n",
        "                    estimatorParamMaps = params,\n",
        "                    evaluator = evaluator,\n",
        "                    numFolds = 10,\n",
        "                    seed = 13)\n",
        "cv = cv.fit(cars_train)\n",
        "\n",
        "# RMSE\n",
        "cv.avgMetrics\n",
        "\n",
        "evaluator.evaluate(cv.transform(cars_test))\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "# Create an empty parameter grid\n",
        "params = ParamGridBuilder().build()\n",
        "\n",
        "# Create objects for building and evaluating a regression model\n",
        "regression = LinearRegression(labelCol = 'duration')\n",
        "evaluator = RegressionEvaluator(labelCol = 'duration')\n",
        "\n",
        "# Create a cross validator\n",
        "cv = CrossValidator(estimator= regression, estimatorParamMaps= params, evaluator= evaluator, numFolds = 5)\n",
        "\n",
        "# Train and test model on multiple folds of the training data\n",
        "cv = cv.fit(flights_train)\n",
        "\n",
        "# NOTE: Since cross-valdiation builds multiple models, the fit() method can take a little while to complete.\n",
        "\n",
        "# Create an indexer for the org field\n",
        "indexer = StringIndexer(inputCol = 'org', outputCol = 'org_idx')\n",
        "\n",
        "# Create an one-hot encoder for the indexed org field\n",
        "onehot = OneHotEncoderEstimator(inputCols = ['org_idx'], outputCols = ['org_dummy'])\n",
        "\n",
        "# Assemble the km and one-hot encoded fields\n",
        "assembler = VectorAssembler(inputCols = ['km','org_dummy'], outputCol = 'features')\n",
        "\n",
        "# Create a pipeline and cross-validator.\n",
        "pipeline = Pipeline(stages=[indexer, onehot, assembler, regression])\n",
        "cv = CrossValidator(estimator= pipeline,\n",
        "          estimatorParamMaps= params,\n",
        "          evaluator = evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OPnPfWStIGZ"
      },
      "source": [
        "## **Grid Search**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdIP1v8WtK_4"
      },
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder\n",
        "\n",
        "# Create a parameter grid builder\n",
        "params = ParamGridBuilder()\n",
        "# Add grid points\n",
        "params = params.addGrid(regression.fitIntercept, [True, False])\n",
        "# Construct the grid: This is useful because it allow build a model for each point in the grid\n",
        "params = params.build()\n",
        "\n",
        "# How many models?\n",
        "print('Number of models to be tested:', len(params))\n",
        "\n",
        "cv = CrossValidator( estimator = regression,\n",
        "                    estimatorParamMaps = params,\n",
        "                    evaluator = evaluator)\n",
        "cv = cv.setNumFolds(10).setSeed(13).fit(cars_train)\n",
        "cv.avgMetrics\n",
        "\n",
        "# Access the best model\n",
        "cv.bestModel\n",
        "\n",
        "# Predictions\n",
        "predictions = cv.transform(cars_test)\n",
        "\n",
        "# Retrieve the best parameter\n",
        "cv.bestModel.explainParam('fitIntercept')\n",
        "\n",
        "\n",
        "### More complicated Grid\n",
        "params = ParamGridBuilder() \\\n",
        "                .addGrid(regression.fitIntercept, [True, False]) \\\n",
        "                .addGrid(regression.regParam, [0.001, 0.01,0.1,1,10]) \\\n",
        "                .addGrid(regression.elasticNetParam, [0,0.25,0.5,0.75,1]) \\\n",
        "                .build()\n",
        "\n",
        "# How many models now?\n",
        "print('Number of models to be tested:', len(params))\n",
        "\n",
        "###\n",
        "# Create parameter grid\n",
        "params = ParamGridBuilder()\n",
        "\n",
        "# Add grids for two parameters\n",
        "params = params.addGrid(regression.regParam,[0.01,0.1,1.0,10.0]) \\\n",
        "               .addGrid(regression.elasticNetParam,[0.0,0.5,1.0])\n",
        "\n",
        "# Build the parameter grid\n",
        "params = params.build()\n",
        "print('Number of models to be tested: ', len(params))\n",
        "\n",
        "# Create cross-validator\n",
        "cv = CrossValidator(estimator= pipeline, estimatorParamMaps= params, evaluator= evaluator, numFolds = 5)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}